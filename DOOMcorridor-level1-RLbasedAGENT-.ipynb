{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "750fcee2-8073-4ab5-91e2-51975be22320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: vizdoom in c:\\users\\indug\\anaconda3\\lib\\site-packages (1.2.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\indug\\anaconda3\\lib\\site-packages (from vizdoom) (1.26.4)\n",
      "Requirement already satisfied: gymnasium>=0.28.0 in c:\\users\\indug\\anaconda3\\lib\\site-packages (from vizdoom) (0.29.1)\n",
      "Requirement already satisfied: pygame>=2.1.3 in c:\\users\\indug\\anaconda3\\lib\\site-packages (from vizdoom) (2.5.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\indug\\anaconda3\\lib\\site-packages (from gymnasium>=0.28.0->vizdoom) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\indug\\anaconda3\\lib\\site-packages (from gymnasium>=0.28.0->vizdoom) (4.9.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\indug\\anaconda3\\lib\\site-packages (from gymnasium>=0.28.0->vizdoom) (0.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install vizdoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "492c2ee9-0eaf-4374-8cd2-c191bd4993b5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The system cannot find the path specified.\n",
      "Cloning into 'ViZDoom'...\n",
      "Updating files:   8% (174/2062)\n",
      "Updating files:   9% (186/2062)\n",
      "Updating files:  10% (207/2062)\n",
      "Updating files:  11% (227/2062)\n",
      "Updating files:  12% (248/2062)\n",
      "Updating files:  13% (269/2062)\n",
      "Updating files:  14% (289/2062)\n",
      "Updating files:  15% (310/2062)\n",
      "Updating files:  16% (330/2062)\n",
      "Updating files:  17% (351/2062)\n",
      "Updating files:  18% (372/2062)\n",
      "Updating files:  19% (392/2062)\n",
      "Updating files:  20% (413/2062)\n",
      "Updating files:  21% (434/2062)\n",
      "Updating files:  22% (454/2062)\n",
      "Updating files:  23% (475/2062)\n",
      "Updating files:  24% (495/2062)\n",
      "Updating files:  25% (516/2062)\n",
      "Updating files:  26% (537/2062)\n",
      "Updating files:  27% (557/2062)\n",
      "Updating files:  28% (578/2062)\n",
      "Updating files:  29% (598/2062)\n",
      "Updating files:  30% (619/2062)\n",
      "Updating files:  31% (640/2062)\n",
      "Updating files:  32% (660/2062)\n",
      "Updating files:  33% (681/2062)\n",
      "Updating files:  34% (702/2062)\n",
      "Updating files:  35% (722/2062)\n",
      "Updating files:  36% (743/2062)\n",
      "Updating files:  37% (763/2062)\n",
      "Updating files:  38% (784/2062)\n",
      "Updating files:  39% (805/2062)\n",
      "Updating files:  40% (825/2062)\n",
      "Updating files:  41% (846/2062)\n",
      "Updating files:  42% (867/2062)\n",
      "Updating files:  42% (873/2062)\n",
      "Updating files:  43% (887/2062)\n",
      "Updating files:  44% (908/2062)\n",
      "Updating files:  45% (928/2062)\n",
      "Updating files:  46% (949/2062)\n",
      "Updating files:  47% (970/2062)\n",
      "Updating files:  48% (990/2062)\n",
      "Updating files:  49% (1011/2062)\n",
      "Updating files:  50% (1031/2062)\n",
      "Updating files:  51% (1052/2062)\n",
      "Updating files:  52% (1073/2062)\n",
      "Updating files:  53% (1093/2062)\n",
      "Updating files:  54% (1114/2062)\n",
      "Updating files:  55% (1135/2062)\n",
      "Updating files:  56% (1155/2062)\n",
      "Updating files:  57% (1176/2062)\n",
      "Updating files:  58% (1196/2062)\n",
      "Updating files:  59% (1217/2062)\n",
      "Updating files:  60% (1238/2062)\n",
      "Updating files:  61% (1258/2062)\n",
      "Updating files:  62% (1279/2062)\n",
      "Updating files:  63% (1300/2062)\n",
      "Updating files:  64% (1320/2062)\n",
      "Updating files:  65% (1341/2062)\n",
      "Updating files:  66% (1361/2062)\n",
      "Updating files:  67% (1382/2062)\n",
      "Updating files:  68% (1403/2062)\n",
      "Updating files:  69% (1423/2062)\n",
      "Updating files:  70% (1444/2062)\n",
      "Updating files:  71% (1465/2062)\n",
      "Updating files:  72% (1485/2062)\n",
      "Updating files:  73% (1506/2062)\n",
      "Updating files:  74% (1526/2062)\n",
      "Updating files:  75% (1547/2062)\n",
      "Updating files:  76% (1568/2062)\n",
      "Updating files:  76% (1582/2062)\n",
      "Updating files:  77% (1588/2062)\n",
      "Updating files:  78% (1609/2062)\n",
      "Updating files:  79% (1629/2062)\n",
      "Updating files:  80% (1650/2062)\n",
      "Updating files:  81% (1671/2062)\n",
      "Updating files:  82% (1691/2062)\n",
      "Updating files:  83% (1712/2062)\n",
      "Updating files:  84% (1733/2062)\n",
      "Updating files:  85% (1753/2062)\n",
      "Updating files:  86% (1774/2062)\n",
      "Updating files:  87% (1794/2062)\n",
      "Updating files:  88% (1815/2062)\n",
      "Updating files:  89% (1836/2062)\n",
      "Updating files:  90% (1856/2062)\n",
      "Updating files:  91% (1877/2062)\n",
      "Updating files:  92% (1898/2062)\n",
      "Updating files:  93% (1918/2062)\n",
      "Updating files:  94% (1939/2062)\n",
      "Updating files:  95% (1959/2062)\n",
      "Updating files:  96% (1980/2062)\n",
      "Updating files:  97% (2001/2062)\n",
      "Updating files:  98% (2021/2062)\n",
      "Updating files:  99% (2042/2062)\n",
      "Updating files: 100% (2062/2062)\n",
      "Updating files: 100% (2062/2062), done.\n"
     ]
    }
   ],
   "source": [
    "# !cd github & git clone https://github.com/mwydmuch/ViZDoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f52ae29b-6a2a-4e75-b260-4736691d846b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import vizdoom for game env\n",
    "from vizdoom import * \n",
    "# Import random for action sampling\n",
    "import random\n",
    "# Import time for sleeping\n",
    "import time \n",
    "# Import numpy for identity matrix\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22894e4e-61c4-4cbe-8890-cd36f34edbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup game\n",
    "game = DoomGame()\n",
    "game.load_config(r'C:\\Users\\indug\\doom\\ViZDoom\\scenarios\\deadly_corridor_s1.cfg')\n",
    "game.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3687adf0-dea8-4966-8c74-bf312a636999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the set of actions we can take in the environment\n",
    "actions = np.identity(7, dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcfd165d-47a1-4c2c-8e03-2d0d4e11ca61",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = game.get_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f433d6e6-a66a-4112-8f70-9284e7b33c3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([100.,   0.,   0.,  -1.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.game_variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b37eb032-de5a-4c42-89ca-83f9dbe3994a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 7.1137542724609375\n",
      "reward: 8.533950805664062\n",
      "reward: 3.7769317626953125\n",
      "reward: 8.105987548828125\n",
      "reward: 9.203216552734375\n",
      "reward: 13.321395874023438\n",
      "reward: 12.7210693359375\n",
      "reward: 5.0521392822265625\n",
      "reward: 1.044891357421875\n",
      "reward: -3.2614593505859375\n",
      "reward: -12.4375\n",
      "reward: -11.914398193359375\n",
      "reward: -11.06475830078125\n",
      "reward: -12.229812622070312\n",
      "reward: -8.249313354492188\n",
      "reward: -5.564422607421875\n",
      "reward: -9.046905517578125\n",
      "reward: -7.876251220703125\n",
      "reward: -3.1651611328125\n",
      "reward: 7.0593719482421875\n",
      "reward: 8.468719482421875\n",
      "reward: 3.0680389404296875\n",
      "reward: 0.145294189453125\n",
      "reward: 0.0978546142578125\n",
      "reward: 0.0659332275390625\n",
      "reward: 0.0345611572265625\n",
      "reward: 0.0\n",
      "reward: 3.5823974609375\n",
      "reward: -1.7559661865234375\n",
      "reward: -7.946044921875\n",
      "reward: -1.18780517578125\n",
      "reward: 2.3775634765625\n",
      "reward: 1.6036224365234375\n",
      "reward: 1.0815582275390625\n",
      "reward: 2.184722900390625\n",
      "reward: 8.394332885742188\n",
      "reward: 9.311203002929688\n",
      "reward: 13.229446411132812\n",
      "reward: 11.117111206054688\n",
      "reward: 8.189544677734375\n",
      "reward: 4.8327178955078125\n",
      "reward: 2.135040283203125\n",
      "reward: 0.607391357421875\n",
      "reward: -6.6499481201171875\n",
      "reward: -10.523025512695312\n",
      "reward: -5.0574493408203125\n",
      "reward: -1.57330322265625\n",
      "reward: -0.6223297119140625\n",
      "reward: -0.419891357421875\n",
      "reward: 6.776123046875\n",
      "reward: 15.33709716796875\n",
      "reward: 14.888092041015625\n",
      "reward: 5.6678314208984375\n",
      "reward: 9.712615966796875\n",
      "reward: 9.091644287109375\n",
      "reward: 5.6959686279296875\n",
      "reward: 9.272857666015625\n",
      "reward: 2.2204132080078125\n",
      "reward: -9.270278930664062\n",
      "reward: -17.02099609375\n",
      "reward: -22.248947143554688\n",
      "reward: -18.714950561523438\n",
      "reward: -12.623626708984375\n",
      "reward: -8.457901000976562\n",
      "reward: -4.512176513671875\n",
      "reward: -4.8461456298828125\n",
      "reward: -3.7109527587890625\n",
      "reward: -0.91961669921875\n",
      "reward: -1.1134490966796875\n",
      "reward: 0.2692718505859375\n",
      "reward: 0.8368988037109375\n",
      "reward: 0.564361572265625\n",
      "reward: 0.380523681640625\n",
      "reward: 0.2015228271484375\n",
      "reward: 0.0\n",
      "reward: 0.4185791015625\n",
      "reward: 0.9206695556640625\n",
      "reward: 0.421966552734375\n",
      "reward: 7.16448974609375\n",
      "reward: 8.560897827148438\n",
      "reward: 4.9660186767578125\n",
      "reward: 3.3495941162109375\n",
      "reward: 2.2592620849609375\n",
      "reward: 1.523834228515625\n",
      "reward: 1.0277557373046875\n",
      "reward: 0.8097381591796875\n",
      "reward: 0.424072265625\n",
      "reward: -0.75750732421875\n",
      "reward: -1.0590057373046875\n",
      "reward: -0.71441650390625\n",
      "reward: -0.48199462890625\n",
      "reward: -7.4355316162109375\n",
      "reward: -8.539932250976562\n",
      "reward: 1.4596710205078125\n",
      "reward: 4.71826171875\n",
      "reward: 2.54608154296875\n",
      "reward: -0.1494598388671875\n",
      "reward: -0.4662017822265625\n",
      "reward: 4.97662353515625\n",
      "reward: 5.7014617919921875\n",
      "reward: 3.84564208984375\n",
      "reward: -3.873870849609375\n",
      "reward: -6.009552001953125\n",
      "reward: -4.05364990234375\n",
      "reward: -9.515914916992188\n",
      "reward: -9.979949951171875\n",
      "reward: -6.7317047119140625\n",
      "reward: -4.540771484375\n",
      "reward: 3.40460205078125\n",
      "reward: 5.6927032470703125\n",
      "reward: 7.417694091796875\n",
      "reward: 12.938461303710938\n",
      "reward: 17.963775634765625\n",
      "reward: 15.297149658203125\n",
      "reward: 10.318099975585938\n",
      "reward: 9.798599243164062\n",
      "reward: 8.10003662109375\n",
      "reward: 11.931076049804688\n",
      "reward: 4.97625732421875\n",
      "reward: -2.8791046142578125\n",
      "reward: -3.43304443359375\n",
      "reward: -2.3157806396484375\n",
      "reward: 0.694000244140625\n",
      "reward: -0.6034393310546875\n",
      "reward: -1.591949462890625\n",
      "reward: -4.1021270751953125\n",
      "reward: -2.72705078125\n",
      "reward: 2.322906494140625\n",
      "reward: 3.538482666015625\n",
      "reward: -0.6415252685546875\n",
      "reward: 4.3506011962890625\n",
      "reward: 9.309677124023438\n",
      "reward: 7.8696441650390625\n",
      "reward: -1.065643310546875\n",
      "reward: -4.066009521484375\n",
      "reward: 3.1961669921875\n",
      "reward: 5.2744903564453125\n",
      "reward: 8.972549438476562\n",
      "reward: 8.8956298828125\n",
      "reward: -1.222320556640625\n",
      "reward: 0.836273193359375\n",
      "reward: 8.117828369140625\n",
      "reward: 3.087646484375\n",
      "reward: -0.55804443359375\n",
      "reward: -0.1050567626953125\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: -5.270904541015625\n",
      "reward: -11.594223022460938\n",
      "reward: -10.588485717773438\n",
      "reward: -7.1421966552734375\n",
      "reward: -5.809326171875\n",
      "reward: -0.383056640625\n",
      "reward: 2.4552154541015625\n",
      "reward: 6.9266815185546875\n",
      "reward: 7.4398956298828125\n",
      "reward: 0.371612548828125\n",
      "reward: 2.4569244384765625\n",
      "reward: 9.264801025390625\n",
      "reward: 8.962844848632812\n",
      "reward: 6.04547119140625\n",
      "reward: 6.931488037109375\n",
      "reward: 6.8463897705078125\n",
      "reward: 6.3885650634765625\n",
      "reward: 3.4680938720703125\n",
      "reward: -5.203643798828125\n",
      "reward: -0.369842529296875\n",
      "reward: 1.45196533203125\n",
      "reward: -1.6001129150390625\n",
      "reward: -3.450836181640625\n",
      "reward: -3.11279296875\n",
      "reward: -2.099700927734375\n",
      "reward: -8.18841552734375\n",
      "reward: -9.079605102539062\n",
      "reward: -6.1244049072265625\n",
      "reward: -5.1386871337890625\n",
      "reward: -2.9084320068359375\n",
      "reward: -1.9619140625\n",
      "reward: 4.9230194091796875\n",
      "reward: 6.54931640625\n",
      "reward: 11.244644165039062\n",
      "reward: 17.996902465820312\n",
      "reward: 7.0608367919921875\n",
      "reward: 2.5002899169921875\n",
      "reward: 8.513519287109375\n",
      "reward: 9.158370971679688\n",
      "reward: -0.738677978515625\n",
      "reward: -4.0835113525390625\n",
      "reward: -2.585357666015625\n",
      "reward: -1.6551513671875\n",
      "reward: -1.1165313720703125\n",
      "reward: 6.04248046875\n",
      "reward: 7.644317626953125\n",
      "reward: 5.1561126708984375\n",
      "reward: 10.112319946289062\n",
      "reward: 3.6703338623046875\n",
      "reward: 5.6260986328125\n",
      "reward: 7.278839111328125\n",
      "reward: 6.2530517578125\n",
      "reward: 10.852676391601562\n",
      "reward: 10.804229736328125\n",
      "reward: 0.9010009765625\n",
      "reward: -2.74615478515625\n",
      "reward: 0.1565704345703125\n",
      "reward: 1.0382232666015625\n",
      "reward: -0.121917724609375\n",
      "reward: 0.3079376220703125\n",
      "reward: 0.639251708984375\n",
      "reward: 0.43109130859375\n",
      "reward: -1.3900909423828125\n",
      "reward: -0.1397857666015625\n",
      "reward: -0.89263916015625\n",
      "reward: -1.4848480224609375\n",
      "reward: -1.0016632080078125\n",
      "reward: -4.499847412109375\n",
      "reward: 4.2083282470703125\n",
      "reward: 5.578399658203125\n",
      "reward: 3.76263427734375\n",
      "reward: 2.5378570556640625\n",
      "reward: 1.711700439453125\n",
      "reward: 1.154449462890625\n",
      "reward: 0.791046142578125\n",
      "reward: -5.058685302734375\n",
      "reward: 2.899505615234375\n",
      "reward: 5.5418243408203125\n",
      "reward: 3.73797607421875\n",
      "reward: 2.5212554931640625\n",
      "reward: 8.1287841796875\n",
      "reward: 11.167160034179688\n",
      "reward: 6.60394287109375\n",
      "reward: 1.2559356689453125\n",
      "reward: -0.8325347900390625\n",
      "reward: -0.5616607666015625\n",
      "reward: -0.378997802734375\n",
      "reward: -6.610198974609375\n",
      "reward: -7.7957916259765625\n",
      "reward: -1.8668060302734375\n",
      "reward: 0.5217742919921875\n",
      "reward: 6.3520965576171875\n",
      "reward: 7.435455322265625\n",
      "reward: 5.015228271484375\n",
      "reward: -2.1299896240234375\n",
      "reward: -0.0563507080078125\n",
      "reward: -3.391876220703125\n",
      "reward: -0.677490234375\n",
      "reward: -4.6764068603515625\n",
      "reward: -6.0676422119140625\n",
      "reward: -8.741485595703125\n",
      "reward: -13.677978515625\n",
      "reward: -12.377166748046875\n",
      "reward: -4.9570159912109375\n",
      "reward: -1.5521087646484375\n",
      "reward: -0.5601348876953125\n",
      "reward: 2.8209075927734375\n",
      "reward: 3.5301055908203125\n",
      "reward: 2.3810272216796875\n",
      "reward: 1.6059112548828125\n",
      "reward: 2.6888275146484375\n",
      "reward: 2.635711669921875\n",
      "reward: 1.7776947021484375\n",
      "reward: 1.1989898681640625\n",
      "reward: 0.808624267578125\n",
      "reward: 0.5453338623046875\n",
      "reward: -4.2788848876953125\n",
      "reward: -10.494171142578125\n",
      "reward: -14.438980102539062\n",
      "reward: -12.179534912109375\n",
      "reward: -8.21539306640625\n",
      "reward: -11.216903686523438\n",
      "reward: -6.594390869140625\n",
      "reward: -6.3249053955078125\n",
      "reward: -12.017242431640625\n",
      "reward: -11.0863037109375\n",
      "reward: -5.97991943359375\n",
      "reward: -0.5045928955078125\n",
      "reward: 4.961639404296875\n",
      "reward: 10.609832763671875\n",
      "reward: 9.508468627929688\n",
      "reward: 6.6717681884765625\n",
      "reward: 9.375885009765625\n",
      "reward: 2.6592864990234375\n",
      "reward: 3.31915283203125\n",
      "reward: 4.4501953125\n",
      "reward: -0.76727294921875\n",
      "reward: -2.496856689453125\n",
      "reward: -1.684295654296875\n",
      "reward: -1.1361541748046875\n",
      "reward: -0.7664337158203125\n",
      "reward: 5.2989654541015625\n",
      "reward: 10.556365966796875\n",
      "reward: 13.00274658203125\n",
      "reward: 7.307861328125\n",
      "reward: 7.9833526611328125\n",
      "reward: 5.3847503662109375\n",
      "reward: 3.6320037841796875\n",
      "reward: 8.61761474609375\n",
      "reward: 9.051589965820312\n",
      "reward: 6.1053314208984375\n",
      "reward: 1.2721099853515625\n",
      "reward: -0.5850372314453125\n",
      "reward: -0.3946990966796875\n",
      "reward: -0.266326904296875\n",
      "reward: -5.4194488525390625\n",
      "reward: -2.49798583984375\n",
      "reward: 0.311737060546875\n",
      "reward: 3.104522705078125\n",
      "reward: 3.585693359375\n",
      "reward: 5.948638916015625\n",
      "reward: 11.624725341796875\n",
      "reward: 16.670166015625\n",
      "reward: 8.495025634765625\n",
      "reward: 6.45849609375\n",
      "reward: 0.572601318359375\n",
      "reward: -0.6606903076171875\n",
      "reward: -0.4457550048828125\n",
      "reward: 5.8881988525390625\n",
      "reward: 7.2216644287109375\n",
      "reward: 10.660140991210938\n",
      "reward: 4.441162109375\n",
      "reward: -100.64862060546875\n",
      "Result: 267.1562957763672\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: -6.710723876953125\n",
      "reward: -9.208419799804688\n",
      "reward: 6.7105255126953125\n",
      "reward: 8.0501708984375\n",
      "reward: 7.6909637451171875\n",
      "reward: 6.375\n",
      "reward: 4.299957275390625\n",
      "reward: 3.3235626220703125\n",
      "reward: -5.9127655029296875\n",
      "reward: -8.745559692382812\n",
      "reward: -8.118789672851562\n",
      "reward: -7.19952392578125\n",
      "reward: -3.6864776611328125\n",
      "reward: -2.694580078125\n",
      "reward: -0.149505615234375\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: -0.02099609375\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 6.000274658203125\n",
      "reward: 1.1977386474609375\n",
      "reward: -2.3432769775390625\n",
      "reward: 1.0200042724609375\n",
      "reward: 8.40802001953125\n",
      "reward: 9.007980346679688\n",
      "reward: -0.27886962890625\n",
      "reward: -3.52484130859375\n",
      "reward: -4.148468017578125\n",
      "reward: -3.7282257080078125\n",
      "reward: 3.839569091796875\n",
      "reward: 5.9266815185546875\n",
      "reward: 7.38922119140625\n",
      "reward: 6.76513671875\n",
      "reward: 10.56341552734375\n",
      "reward: 13.66778564453125\n",
      "reward: 14.391815185546875\n",
      "reward: 17.340301513671875\n",
      "reward: 15.425445556640625\n",
      "reward: 11.53485107421875\n",
      "reward: 0.15936279296875\n",
      "reward: 1.3026123046875\n",
      "reward: 2.9822540283203125\n",
      "reward: 2.011474609375\n",
      "reward: 6.38494873046875\n",
      "reward: 6.9471588134765625\n",
      "reward: 0.55401611328125\n",
      "reward: -5.86676025390625\n",
      "reward: -0.1535797119140625\n",
      "reward: -3.490234375\n",
      "reward: -10.82843017578125\n",
      "reward: -10.115097045898438\n",
      "reward: -2.0093231201171875\n",
      "reward: 1.562347412109375\n",
      "reward: -3.0780487060546875\n",
      "reward: -8.300201416015625\n",
      "reward: -1.1861572265625\n",
      "reward: -0.8001708984375\n",
      "reward: -0.5398101806640625\n",
      "reward: -5.3692779541015625\n",
      "reward: -2.187744140625\n",
      "reward: -3.329193115234375\n",
      "reward: -8.23663330078125\n",
      "reward: -7.6186065673828125\n",
      "reward: -4.246429443359375\n",
      "reward: -2.194061279296875\n",
      "reward: -1.480010986328125\n",
      "reward: 3.4116363525390625\n",
      "reward: -0.66143798828125\n",
      "reward: -7.7097320556640625\n",
      "reward: -12.287139892578125\n",
      "reward: -10.728012084960938\n",
      "reward: -0.993621826171875\n",
      "reward: 7.771514892578125\n",
      "reward: 12.6470947265625\n",
      "reward: 10.991790771484375\n",
      "reward: 2.2462921142578125\n",
      "reward: -5.8453216552734375\n",
      "reward: -9.54730224609375\n",
      "reward: -0.3461151123046875\n",
      "reward: 6.8529205322265625\n",
      "reward: 7.06231689453125\n",
      "reward: 7.7332916259765625\n",
      "reward: 5.7378692626953125\n",
      "reward: 3.870208740234375\n",
      "reward: 2.6104583740234375\n",
      "reward: 1.7606964111328125\n",
      "reward: 1.187530517578125\n",
      "reward: 4.5556640625\n",
      "reward: 10.983413696289062\n",
      "reward: 16.466033935546875\n",
      "reward: 14.1298828125\n",
      "reward: 0.8439178466796875\n",
      "reward: 0.569122314453125\n",
      "reward: 4.138519287109375\n",
      "reward: 8.51788330078125\n",
      "reward: 13.655990600585938\n",
      "reward: 12.329788208007812\n",
      "reward: 8.316497802734375\n",
      "reward: 5.5718994140625\n",
      "reward: 3.5001068115234375\n",
      "reward: 2.3607177734375\n",
      "reward: 5.669525146484375\n",
      "reward: 9.849380493164062\n",
      "reward: 14.233535766601562\n",
      "reward: 12.517227172851562\n",
      "reward: 8.44293212890625\n",
      "reward: 5.6947784423828125\n",
      "reward: 1.002044677734375\n",
      "reward: -7.2828216552734375\n",
      "reward: -8.308914184570312\n",
      "reward: -4.8195648193359375\n",
      "reward: -9.456161499023438\n",
      "reward: -11.905990600585938\n",
      "reward: -9.471588134765625\n",
      "reward: -7.0232391357421875\n",
      "reward: -6.3334503173828125\n",
      "reward: -3.0473175048828125\n",
      "reward: -7.7566680908203125\n",
      "reward: -8.793350219726562\n",
      "reward: -5.93133544921875\n",
      "reward: -0.9251556396484375\n",
      "reward: -1.8192138671875\n",
      "reward: -9.185806274414062\n",
      "reward: -9.592498779296875\n",
      "reward: -6.47039794921875\n",
      "reward: -2.3070220947265625\n",
      "reward: 6.3056182861328125\n",
      "reward: 14.595779418945312\n",
      "reward: 14.569259643554688\n",
      "reward: 1.3863677978515625\n",
      "reward: -8.839080810546875\n",
      "reward: -14.846038818359375\n",
      "reward: -15.020706176757812\n",
      "reward: -17.993820190429688\n",
      "reward: -15.698501586914062\n",
      "reward: -10.5889892578125\n",
      "reward: -9.981643676757812\n",
      "reward: -10.027816772460938\n",
      "reward: -5.085540771484375\n",
      "reward: -3.4304046630859375\n",
      "reward: -2.3139801025390625\n",
      "reward: 1.2780303955078125\n",
      "reward: 2.352783203125\n",
      "reward: -1.9911041259765625\n",
      "reward: 0.3559112548828125\n",
      "reward: 8.175216674804688\n",
      "reward: 12.272598266601562\n",
      "reward: 4.10052490234375\n",
      "reward: -4.667449951171875\n",
      "reward: 0.5259857177734375\n",
      "reward: 3.535064697265625\n",
      "reward: -0.4547576904296875\n",
      "reward: 4.6697845458984375\n",
      "reward: 3.0329132080078125\n",
      "reward: 3.9817657470703125\n",
      "reward: 4.1764984130859375\n",
      "reward: 2.8170013427734375\n",
      "reward: 1.8999786376953125\n",
      "reward: 1.2814483642578125\n",
      "reward: 3.0876922607421875\n",
      "reward: 0.5077362060546875\n",
      "reward: -5.3769378662109375\n",
      "reward: -12.13482666015625\n",
      "reward: -11.020339965820312\n",
      "reward: -4.9759521484375\n",
      "reward: -2.1047210693359375\n",
      "reward: 5.2926483154296875\n",
      "reward: 7.094818115234375\n",
      "reward: 7.041595458984375\n",
      "reward: 4.4276580810546875\n",
      "reward: 5.1486663818359375\n",
      "reward: 4.8025665283203125\n",
      "reward: 3.2392730712890625\n",
      "reward: 1.3541412353515625\n",
      "reward: 0.4770355224609375\n",
      "reward: 1.1523590087890625\n",
      "reward: 8.273544311523438\n",
      "reward: 8.45733642578125\n",
      "reward: 6.0989990234375\n",
      "reward: 5.3807525634765625\n",
      "reward: 8.151885986328125\n",
      "reward: -0.177886962890625\n",
      "reward: -2.678619384765625\n",
      "reward: -2.0914306640625\n",
      "reward: -1.8470458984375\n",
      "reward: -1.246002197265625\n",
      "reward: -0.840545654296875\n",
      "reward: -0.379241943359375\n",
      "reward: 6.904266357421875\n",
      "reward: 8.392684936523438\n",
      "reward: 5.6609039306640625\n",
      "reward: -2.8924560546875\n",
      "reward: -5.47515869140625\n",
      "reward: -3.693206787109375\n",
      "reward: -4.75250244140625\n",
      "reward: -11.103927612304688\n",
      "reward: -11.013931274414062\n",
      "reward: -7.736663818359375\n",
      "reward: -3.124542236328125\n",
      "reward: 0.1534881591796875\n",
      "reward: 1.290802001953125\n",
      "reward: 0.8704986572265625\n",
      "reward: 0.5870513916015625\n",
      "reward: 0.3958587646484375\n",
      "reward: 6.2027435302734375\n",
      "reward: 11.059951782226562\n",
      "reward: 3.5055999755859375\n",
      "reward: -0.4271392822265625\n",
      "reward: -0.2882080078125\n",
      "reward: 3.4533538818359375\n",
      "reward: 0.345672607421875\n",
      "reward: 2.01800537109375\n",
      "reward: 7.0270233154296875\n",
      "reward: 5.78753662109375\n",
      "reward: 8.686141967773438\n",
      "reward: 13.414947509765625\n",
      "reward: 12.165618896484375\n",
      "reward: 8.205825805664062\n",
      "reward: 5.53485107421875\n",
      "reward: 3.7332305908203125\n",
      "reward: -1.241180419921875\n",
      "reward: -2.8114166259765625\n",
      "reward: -7.8323822021484375\n",
      "reward: -4.641326904296875\n",
      "reward: -1.1567230224609375\n",
      "reward: -0.78033447265625\n",
      "reward: -0.526458740234375\n",
      "reward: 6.0159149169921875\n",
      "reward: 13.774520874023438\n",
      "reward: 12.636749267578125\n",
      "reward: 8.523605346679688\n",
      "reward: 3.48797607421875\n",
      "reward: 0.875274658203125\n",
      "reward: 0.5211334228515625\n",
      "reward: -1.220794677734375\n",
      "reward: -1.6239166259765625\n",
      "reward: -1.095428466796875\n",
      "reward: 5.6923828125\n",
      "reward: 7.171234130859375\n",
      "reward: 4.791351318359375\n",
      "reward: -3.9029541015625\n",
      "reward: 0.74530029296875\n",
      "reward: 4.2382965087890625\n",
      "reward: 9.972457885742188\n",
      "reward: 17.575958251953125\n",
      "reward: 8.477066040039062\n",
      "reward: 1.9688262939453125\n",
      "reward: -6.4236297607421875\n",
      "reward: -8.068649291992188\n",
      "reward: -5.4425201416015625\n",
      "reward: -3.6711883544921875\n",
      "reward: -1.640533447265625\n",
      "reward: -0.6677703857421875\n",
      "reward: -0.4505462646484375\n",
      "reward: 6.595062255859375\n",
      "reward: 14.970428466796875\n",
      "reward: 13.720672607421875\n",
      "reward: 2.3554534912109375\n",
      "reward: -8.933609008789062\n",
      "reward: -7.9449615478515625\n",
      "reward: -2.7049713134765625\n",
      "reward: -1.0368194580078125\n",
      "reward: 0.1718597412109375\n",
      "reward: 0.1157989501953125\n",
      "reward: -5.78778076171875\n",
      "reward: 0.3400421142578125\n",
      "reward: 7.1054534912109375\n",
      "reward: 3.00750732421875\n",
      "reward: -3.7047576904296875\n",
      "reward: -10.409088134765625\n",
      "reward: -10.138427734375\n",
      "reward: -0.467529296875\n",
      "reward: 6.0630645751953125\n",
      "reward: 5.6822357177734375\n",
      "reward: 3.832672119140625\n",
      "reward: 9.644515991210938\n",
      "reward: 17.271865844726562\n",
      "reward: 15.357192993164062\n",
      "reward: 3.244781494140625\n",
      "reward: -1.5471649169921875\n",
      "reward: -1.0437469482421875\n",
      "reward: -7.764434814453125\n",
      "reward: -8.944931030273438\n",
      "reward: -6.03363037109375\n",
      "reward: -4.0698394775390625\n",
      "reward: -2.7452392578125\n",
      "reward: -1.851806640625\n",
      "reward: 4.804229736328125\n",
      "reward: 0.3657684326171875\n",
      "reward: 0.6501007080078125\n",
      "reward: -3.73382568359375\n",
      "reward: -5.69757080078125\n",
      "reward: 3.0781097412109375\n",
      "reward: -0.8813323974609375\n",
      "reward: -3.989776611328125\n",
      "reward: 0.8846282958984375\n",
      "reward: 5.5966796875\n",
      "reward: 11.46435546875\n",
      "reward: 10.911666870117188\n",
      "reward: 13.413330078125\n",
      "reward: 6.1728057861328125\n",
      "reward: 0.9846649169921875\n",
      "reward: 6.717437744140625\n",
      "reward: 11.292190551757812\n",
      "reward: 15.551300048828125\n",
      "reward: 13.668411254882812\n",
      "reward: 9.219482421875\n",
      "reward: 13.167572021484375\n",
      "reward: 12.530776977539062\n",
      "reward: 8.452102661132812\n",
      "reward: 5.7009735107421875\n",
      "reward: 0.850433349609375\n",
      "reward: -1.1103973388671875\n",
      "reward: -0.7491302490234375\n",
      "reward: -3.49871826171875\n",
      "reward: -3.881805419921875\n",
      "reward: -4.761932373046875\n",
      "reward: -4.31378173828125\n",
      "reward: -2.9097900390625\n",
      "reward: -1.9628143310546875\n",
      "reward: 0.537567138671875\n",
      "reward: -5.50274658203125\n",
      "reward: -5.44354248046875\n",
      "reward: -1.9657440185546875\n",
      "reward: 0.724700927734375\n",
      "reward: 1.42431640625\n",
      "reward: 1.87725830078125\n",
      "reward: 1.7361907958984375\n",
      "reward: 2.83984375\n",
      "reward: 4.32159423828125\n",
      "reward: 1.8548126220703125\n",
      "reward: 7.11602783203125\n",
      "reward: 10.254806518554688\n",
      "reward: 9.7562255859375\n",
      "reward: 7.5076141357421875\n",
      "reward: -3.1765899658203125\n",
      "reward: -5.5826416015625\n",
      "reward: -3.76568603515625\n",
      "reward: -2.540069580078125\n",
      "reward: -1.71343994140625\n",
      "reward: -1.1558685302734375\n",
      "reward: -0.779754638671875\n",
      "reward: -0.1035003662109375\n",
      "reward: 0.04296875\n",
      "reward: 0.0\n",
      "reward: 6.992431640625\n",
      "reward: 9.50347900390625\n",
      "reward: 6.8344573974609375\n",
      "reward: 4.6098480224609375\n",
      "reward: 3.1093292236328125\n",
      "reward: 4.0727691650390625\n",
      "reward: 3.7603607177734375\n",
      "reward: 2.536346435546875\n",
      "reward: 1.7107086181640625\n",
      "reward: -0.3016204833984375\n",
      "reward: -0.967864990234375\n",
      "reward: 1.608123779296875\n",
      "reward: 4.5330963134765625\n",
      "reward: -2.9907989501953125\n",
      "reward: -5.2675628662109375\n",
      "reward: 3.1574249267578125\n",
      "reward: -1.0571136474609375\n",
      "reward: -1.928314208984375\n",
      "reward: -2.28680419921875\n",
      "reward: 3.980499267578125\n",
      "reward: 3.9474945068359375\n",
      "reward: 5.9641876220703125\n",
      "reward: 12.000961303710938\n",
      "reward: 16.625030517578125\n",
      "reward: 19.371795654296875\n",
      "reward: 15.8255615234375\n",
      "reward: 10.67449951171875\n",
      "reward: 7.2000274658203125\n",
      "reward: 4.7331085205078125\n",
      "reward: 3.136505126953125\n",
      "reward: 9.229293823242188\n",
      "reward: 9.707962036132812\n",
      "reward: 6.433319091796875\n",
      "reward: 5.0419464111328125\n",
      "reward: -3.1875762939453125\n",
      "reward: 1.406982421875\n",
      "reward: 11.716629028320312\n",
      "reward: 11.610504150390625\n",
      "reward: 7.8313751220703125\n",
      "reward: 6.6796722412109375\n",
      "reward: -5.5218048095703125\n",
      "reward: -5.3652801513671875\n",
      "reward: -4.2711181640625\n",
      "reward: -3.642608642578125\n",
      "reward: -2.4571075439453125\n",
      "reward: 5.4027099609375\n",
      "reward: 6.4815521240234375\n",
      "reward: -3.65887451171875\n",
      "reward: -6.1756744384765625\n",
      "reward: -4.1657257080078125\n",
      "reward: -9.710433959960938\n",
      "reward: -10.173660278320312\n",
      "reward: -6.8623809814453125\n",
      "reward: 2.4311981201171875\n",
      "reward: 12.407455444335938\n",
      "reward: 12.076461791992188\n",
      "reward: 15.205810546875\n",
      "reward: 6.90374755859375\n",
      "reward: 0.948974609375\n",
      "reward: -6.4737396240234375\n",
      "reward: -8.102447509765625\n",
      "reward: -5.465301513671875\n",
      "reward: -2.855804443359375\n",
      "reward: -2.3208770751953125\n",
      "reward: -9.062118530273438\n",
      "reward: -9.82025146484375\n",
      "reward: -6.6240386962890625\n",
      "reward: -4.4681243896484375\n",
      "reward: -3.0139007568359375\n",
      "reward: -6.4579010009765625\n",
      "reward: -3.95880126953125\n",
      "reward: 3.237518310546875\n",
      "reward: -1.7836456298828125\n",
      "reward: -4.3220367431640625\n",
      "reward: -2.8958892822265625\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 4.80938720703125\n",
      "reward: 10.797760009765625\n",
      "reward: 14.951950073242188\n",
      "reward: 7.56475830078125\n",
      "reward: 8.61212158203125\n",
      "reward: 13.362747192382812\n",
      "reward: 11.653839111328125\n",
      "reward: 3.725372314453125\n",
      "reward: 4.276123046875\n",
      "reward: 5.053924560546875\n",
      "reward: 3.408843994140625\n",
      "reward: 0.31781005859375\n",
      "reward: -0.82623291015625\n",
      "reward: -0.557403564453125\n",
      "reward: -0.3760528564453125\n",
      "reward: -0.1372528076171875\n",
      "reward: 0.0\n",
      "reward: 4.80938720703125\n",
      "reward: 10.797760009765625\n",
      "reward: 9.923675537109375\n",
      "reward: 6.6935882568359375\n",
      "reward: 4.514862060546875\n",
      "reward: 3.0452728271484375\n",
      "reward: 4.856597900390625\n",
      "reward: -1.527313232421875\n",
      "reward: -10.60028076171875\n",
      "reward: -10.7203369140625\n",
      "reward: -8.111343383789062\n",
      "reward: -11.368972778320312\n",
      "reward: -7.183837890625\n",
      "reward: -2.239105224609375\n",
      "reward: 4.481048583984375\n",
      "reward: 1.56732177734375\n",
      "reward: 3.1023712158203125\n",
      "reward: 4.3544464111328125\n",
      "reward: 2.9370574951171875\n",
      "reward: 6.9489593505859375\n",
      "reward: 7.295928955078125\n",
      "reward: 4.921173095703125\n",
      "reward: -2.23468017578125\n",
      "reward: 1.1298065185546875\n",
      "reward: -0.3206787109375\n",
      "reward: -1.6677703857421875\n",
      "reward: -5.0589599609375\n",
      "reward: -1.1510162353515625\n",
      "reward: -2.801025390625\n",
      "reward: -4.1282501220703125\n",
      "reward: -8.841110229492188\n",
      "reward: -9.144088745117188\n",
      "reward: -6.1679229736328125\n",
      "reward: -4.16046142578125\n",
      "reward: 3.7489776611328125\n",
      "reward: 5.971099853515625\n",
      "reward: 4.027496337890625\n",
      "reward: 0.859710693359375\n",
      "reward: -1.9615631103515625\n",
      "reward: -2.1159515380859375\n",
      "reward: -1.4273223876953125\n",
      "reward: 5.2358856201171875\n",
      "reward: 0.1289215087890625\n",
      "reward: -3.3556976318359375\n",
      "reward: -4.910797119140625\n",
      "reward: -4.299468994140625\n",
      "reward: -5.5201263427734375\n",
      "reward: -8.05859375\n",
      "reward: -6.893341064453125\n",
      "reward: -6.83599853515625\n",
      "reward: 0.9217987060546875\n",
      "reward: 6.6031951904296875\n",
      "reward: -0.7114410400390625\n",
      "reward: -3.92254638671875\n",
      "reward: -8.72271728515625\n",
      "reward: -3.75244140625\n",
      "reward: -0.55645751953125\n",
      "reward: -0.3754119873046875\n",
      "reward: -5.571929931640625\n",
      "reward: -6.46380615234375\n",
      "reward: -9.095565795898438\n",
      "reward: -13.357498168945312\n",
      "reward: -16.23223876953125\n",
      "reward: -9.46246337890625\n",
      "reward: -0.6775970458984375\n",
      "reward: 2.02960205078125\n",
      "reward: 6.1042938232421875\n",
      "reward: 1.8685302734375\n",
      "reward: -1.226531982421875\n",
      "reward: -0.827423095703125\n",
      "reward: -0.558258056640625\n",
      "reward: -5.498687744140625\n",
      "reward: -5.75079345703125\n",
      "Result: 499.23252868652344\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: -0.830780029296875\n",
      "reward: -0.539154052734375\n",
      "reward: -7.11376953125\n",
      "reward: -7.459136962890625\n",
      "reward: -0.03472900390625\n",
      "reward: 0.0\n",
      "reward: -0.019561767578125\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 6.1760101318359375\n",
      "reward: 7.385162353515625\n",
      "reward: 6.6405181884765625\n",
      "reward: 5.350372314453125\n",
      "reward: 3.6087799072265625\n",
      "reward: -4.4652099609375\n",
      "reward: -6.635009765625\n",
      "reward: -1.6489105224609375\n",
      "reward: 2.7386322021484375\n",
      "reward: 5.1049652099609375\n",
      "reward: 4.52227783203125\n",
      "reward: 5.8570098876953125\n",
      "reward: 5.3909454345703125\n",
      "reward: 7.0690765380859375\n",
      "reward: 0.5518798828125\n",
      "reward: -2.6214141845703125\n",
      "reward: -6.6623077392578125\n",
      "reward: -6.9970550537109375\n",
      "reward: -4.7197113037109375\n",
      "reward: -3.18365478515625\n",
      "reward: -8.117660522460938\n",
      "reward: -8.528030395507812\n",
      "reward: -5.0846405029296875\n",
      "reward: -1.6416015625\n",
      "reward: 6.842620849609375\n",
      "reward: 8.171661376953125\n",
      "reward: -4.5773468017578125\n",
      "reward: -8.275054931640625\n",
      "reward: -2.2016754150390625\n",
      "reward: 1.861602783203125\n",
      "reward: 2.2332000732421875\n",
      "reward: 1.5062255859375\n",
      "reward: 1.0158843994140625\n",
      "reward: 0.6669158935546875\n",
      "reward: 1.4351654052734375\n",
      "reward: 8.509735107421875\n",
      "reward: 7.1314239501953125\n",
      "reward: 1.630615234375\n",
      "reward: 0.55181884765625\n",
      "reward: -0.67132568359375\n",
      "reward: -6.2389984130859375\n",
      "reward: -6.5096435546875\n",
      "reward: -4.2924957275390625\n",
      "reward: 4.281341552734375\n",
      "reward: 12.48382568359375\n",
      "reward: 10.40496826171875\n",
      "reward: 5.7629241943359375\n",
      "reward: 8.386932373046875\n",
      "reward: 8.87213134765625\n",
      "reward: 13.067825317382812\n",
      "reward: 12.53411865234375\n",
      "reward: 8.454345703125\n",
      "reward: 5.0800628662109375\n",
      "reward: 2.4771728515625\n",
      "reward: 1.32452392578125\n",
      "reward: 5.9788665771484375\n",
      "reward: 5.4465789794921875\n",
      "reward: 2.8748016357421875\n",
      "reward: 1.9389801025390625\n",
      "reward: 1.307769775390625\n",
      "reward: 0.8820343017578125\n",
      "reward: -4.2121429443359375\n",
      "reward: -2.0516815185546875\n",
      "reward: 0.744781494140625\n",
      "reward: -4.929473876953125\n",
      "reward: -6.177490234375\n",
      "reward: 1.26470947265625\n",
      "reward: 3.705291748046875\n",
      "reward: 7.4671173095703125\n",
      "reward: 12.6134033203125\n",
      "reward: 5.9709625244140625\n",
      "reward: 5.6419677734375\n",
      "reward: 1.486053466796875\n",
      "reward: 3.3565673828125\n",
      "reward: 6.742462158203125\n",
      "reward: 6.7699127197265625\n",
      "reward: 4.56634521484375\n",
      "reward: -2.9825286865234375\n",
      "reward: -1.981597900390625\n",
      "reward: 0.1964569091796875\n",
      "reward: 2.1347808837890625\n",
      "reward: 0.3548431396484375\n",
      "reward: -7.406707763671875\n",
      "reward: -8.407241821289062\n",
      "reward: -12.166793823242188\n",
      "reward: -11.618026733398438\n",
      "reward: -1.34088134765625\n",
      "reward: 2.5066375732421875\n",
      "reward: 1.690643310546875\n",
      "reward: 1.140289306640625\n",
      "reward: -0.789581298828125\n",
      "reward: 0.207305908203125\n",
      "reward: 2.5166168212890625\n",
      "reward: 9.17791748046875\n",
      "reward: 9.689056396484375\n",
      "reward: 4.1387481689453125\n",
      "reward: 1.5330047607421875\n",
      "reward: 1.0338897705078125\n",
      "reward: 0.6972503662109375\n",
      "reward: 4.949066162109375\n",
      "reward: 10.992263793945312\n",
      "reward: 10.19866943359375\n",
      "reward: 6.8790435791015625\n",
      "reward: 4.639892578125\n",
      "reward: 8.918670654296875\n",
      "reward: 9.0557861328125\n",
      "reward: 1.5853729248046875\n",
      "reward: -1.4588470458984375\n",
      "reward: 4.0713958740234375\n",
      "reward: 5.3091583251953125\n",
      "reward: 3.5810089111328125\n",
      "reward: -2.598480224609375\n",
      "reward: -4.2983551025390625\n",
      "reward: 1.7776336669921875\n",
      "reward: 3.5704803466796875\n",
      "reward: 2.4082489013671875\n",
      "reward: 6.1032257080078125\n",
      "reward: 10.947586059570312\n",
      "reward: 5.257232666015625\n",
      "reward: 1.1938934326171875\n",
      "reward: -4.3168487548828125\n",
      "reward: -5.601654052734375\n",
      "reward: -9.466171264648438\n",
      "reward: -3.6844940185546875\n",
      "reward: 0.5013580322265625\n",
      "reward: 0.3380279541015625\n",
      "reward: 0.1251220703125\n",
      "reward: -5.3022918701171875\n",
      "reward: -1.058837890625\n",
      "reward: 2.0700531005859375\n",
      "reward: 1.3961944580078125\n",
      "reward: -2.9697418212890625\n",
      "reward: -8.2366943359375\n",
      "reward: -8.5362548828125\n",
      "reward: -1.8059234619140625\n",
      "reward: 0.8571929931640625\n",
      "reward: 5.7457427978515625\n",
      "reward: 6.5892333984375\n",
      "reward: 4.444488525390625\n",
      "reward: 2.9977569580078125\n",
      "reward: 2.021942138671875\n",
      "reward: 7.0390625\n",
      "reward: 7.728179931640625\n",
      "reward: 10.380264282226562\n",
      "reward: 9.71527099609375\n",
      "reward: 11.199462890625\n",
      "reward: 15.1617431640625\n",
      "reward: 0.3983917236328125\n",
      "reward: -4.3638916015625\n",
      "reward: -2.9435882568359375\n",
      "reward: -1.985595703125\n",
      "reward: -1.33941650390625\n",
      "reward: -5.4859619140625\n",
      "reward: -10.689239501953125\n",
      "reward: -1.9576873779296875\n",
      "reward: 6.4404754638671875\n",
      "reward: 12.021194458007812\n",
      "reward: 15.458511352539062\n",
      "reward: 7.5391693115234375\n",
      "reward: 1.7163238525390625\n",
      "reward: 1.1575927734375\n",
      "reward: 0.7807159423828125\n",
      "reward: 5.7971954345703125\n",
      "reward: 6.6780548095703125\n",
      "reward: 4.5044097900390625\n",
      "reward: -3.51727294921875\n",
      "reward: -5.8151092529296875\n",
      "reward: -3.9224853515625\n",
      "reward: -2.6458587646484375\n",
      "reward: -8.340255737304688\n",
      "reward: -11.715530395507812\n",
      "reward: -11.9398193359375\n",
      "reward: -6.7966156005859375\n",
      "reward: -3.19439697265625\n",
      "reward: -2.15478515625\n",
      "reward: -7.62152099609375\n",
      "reward: -8.3800048828125\n",
      "reward: -11.340255737304688\n",
      "reward: -10.636077880859375\n",
      "reward: -7.126251220703125\n",
      "reward: -2.7562255859375\n",
      "reward: 2.7353515625\n",
      "reward: -0.95703125\n",
      "reward: -7.818939208984375\n",
      "reward: -11.89703369140625\n",
      "reward: -14.32061767578125\n",
      "reward: -11.783615112304688\n",
      "reward: -7.9483489990234375\n",
      "reward: -10.663619995117188\n",
      "reward: -9.977310180664062\n",
      "reward: -11.208999633789062\n",
      "reward: -4.607513427734375\n",
      "reward: -0.0420684814453125\n",
      "reward: -5.3022918701171875\n",
      "reward: -10.84002685546875\n",
      "reward: -5.1851043701171875\n",
      "reward: -5.6245574951171875\n",
      "reward: -6.14605712890625\n",
      "reward: -7.3485870361328125\n",
      "reward: -9.864913940429688\n",
      "reward: -12.198715209960938\n",
      "reward: -10.09503173828125\n",
      "reward: -6.809356689453125\n",
      "reward: -4.5930938720703125\n",
      "reward: -3.0982513427734375\n",
      "reward: -2.0899505615234375\n",
      "reward: 4.7149658203125\n",
      "reward: 3.7823944091796875\n",
      "reward: 1.2602996826171875\n",
      "reward: -5.578216552734375\n",
      "reward: -7.1383819580078125\n",
      "reward: -7.21160888671875\n",
      "reward: -2.963775634765625\n",
      "reward: -0.5769805908203125\n",
      "reward: 2.8090057373046875\n",
      "reward: 3.5741119384765625\n",
      "reward: 2.0453643798828125\n",
      "reward: 1.3004150390625\n",
      "reward: 6.4331512451171875\n",
      "reward: 7.2568511962890625\n",
      "reward: 0.7628936767578125\n",
      "reward: -1.6552734375\n",
      "reward: -1.109161376953125\n",
      "reward: 1.3883514404296875\n",
      "reward: 3.958099365234375\n",
      "reward: 3.0533599853515625\n",
      "reward: 2.2614898681640625\n",
      "reward: 2.434051513671875\n",
      "reward: 3.38916015625\n",
      "reward: -3.2616424560546875\n",
      "reward: -5.44439697265625\n",
      "reward: -3.6724700927734375\n",
      "reward: -2.4772796630859375\n",
      "reward: 2.4207611083984375\n",
      "reward: 3.755096435546875\n",
      "reward: 2.5327911376953125\n",
      "reward: 1.7083282470703125\n",
      "reward: 5.8279571533203125\n",
      "reward: 6.3583831787109375\n",
      "reward: 9.8448486328125\n",
      "reward: 13.689834594726562\n",
      "reward: 11.372360229492188\n",
      "reward: 10.6240234375\n",
      "reward: 2.4899444580078125\n",
      "reward: -1.467742919921875\n",
      "reward: -5.1219024658203125\n",
      "reward: -1.49298095703125\n",
      "reward: 5.237213134765625\n",
      "reward: 2.3356170654296875\n",
      "reward: 1.5753173828125\n",
      "reward: 1.0625\n",
      "reward: 0.7165374755859375\n",
      "reward: 0.4832000732421875\n",
      "reward: 0.3258209228515625\n",
      "reward: -3.7325286865234375\n",
      "reward: 1.082122802734375\n",
      "reward: -1.9653472900390625\n",
      "reward: -4.306182861328125\n",
      "reward: -2.9047088623046875\n",
      "reward: -6.056854248046875\n",
      "reward: -2.1399688720703125\n",
      "reward: 4.8055267333984375\n",
      "reward: 3.5911407470703125\n",
      "reward: 1.0081939697265625\n",
      "reward: 5.8018646240234375\n",
      "reward: 6.6030426025390625\n",
      "reward: -0.0252685546875\n",
      "reward: 2.932891845703125\n",
      "reward: 2.9808349609375\n",
      "reward: 1.5649261474609375\n",
      "reward: 1.0554656982421875\n",
      "reward: -4.8929290771484375\n",
      "reward: -9.75885009765625\n",
      "reward: -8.46148681640625\n",
      "reward: -5.7074432373046875\n",
      "reward: -3.8498687744140625\n",
      "reward: -2.5969390869140625\n",
      "reward: 4.9532012939453125\n",
      "reward: 0.156982421875\n",
      "reward: 3.289764404296875\n",
      "reward: 4.3950042724609375\n",
      "reward: 2.2581024169921875\n",
      "reward: 1.52301025390625\n",
      "reward: -4.5119781494140625\n",
      "reward: -8.146011352539062\n",
      "reward: -6.438018798828125\n",
      "reward: -4.3426666259765625\n",
      "reward: -2.9293212890625\n",
      "reward: 0.624664306640625\n",
      "reward: 4.38763427734375\n",
      "reward: 4.296112060546875\n",
      "reward: -3.6467132568359375\n",
      "reward: -11.16619873046875\n",
      "reward: -15.627700805664062\n",
      "reward: -13.248184204101562\n",
      "reward: -8.93621826171875\n",
      "reward: -4.134521484375\n",
      "reward: -7.8720855712890625\n",
      "reward: -10.173416137695312\n",
      "reward: -6.021514892578125\n",
      "reward: 3.4810333251953125\n",
      "reward: 5.8206024169921875\n",
      "reward: 6.526702880859375\n",
      "reward: 0.8236083984375\n",
      "reward: -4.009979248046875\n",
      "reward: -4.0707244873046875\n",
      "reward: -2.4393768310546875\n",
      "reward: 1.7710113525390625\n",
      "reward: 6.0722808837890625\n",
      "reward: 5.7365875244140625\n",
      "reward: -4.3673858642578125\n",
      "reward: -6.0300140380859375\n",
      "reward: -1.616943359375\n",
      "reward: 2.5178985595703125\n",
      "reward: 0.518310546875\n",
      "reward: -0.9090728759765625\n",
      "reward: -3.009857177734375\n",
      "reward: -2.3951873779296875\n",
      "reward: -0.921875\n",
      "reward: -3.459259033203125\n",
      "reward: -0.6124420166015625\n",
      "reward: 1.2663421630859375\n",
      "reward: 0.854095458984375\n",
      "reward: 0.576019287109375\n",
      "reward: 0.3884429931640625\n",
      "reward: 2.4239044189453125\n",
      "reward: 7.7286376953125\n",
      "reward: 8.6241455078125\n",
      "reward: 3.6269989013671875\n",
      "reward: 1.29632568359375\n",
      "reward: 0.8742523193359375\n",
      "reward: -6.0724945068359375\n",
      "reward: -7.473114013671875\n",
      "reward: -4.9171905517578125\n",
      "reward: -3.3168792724609375\n",
      "reward: -2.2374267578125\n",
      "reward: -1.5092926025390625\n",
      "reward: -1.018157958984375\n",
      "reward: -0.6868743896484375\n",
      "reward: -0.4633941650390625\n",
      "reward: -0.3126373291015625\n",
      "reward: 6.0854949951171875\n",
      "reward: 13.199172973632812\n",
      "reward: 5.119293212890625\n",
      "reward: -0.115814208984375\n",
      "reward: 0.61883544921875\n",
      "reward: 0.086181640625\n",
      "reward: -0.308074951171875\n",
      "reward: -0.2079315185546875\n",
      "reward: 6.56463623046875\n",
      "reward: 7.94891357421875\n",
      "reward: 10.594741821289062\n",
      "reward: 6.9194488525390625\n",
      "reward: 6.857025146484375\n",
      "reward: 5.775054931640625\n",
      "reward: 3.895263671875\n",
      "reward: 3.9721221923828125\n",
      "reward: 3.657012939453125\n",
      "reward: 1.3687896728515625\n",
      "reward: 0.2169189453125\n",
      "reward: 0.146240234375\n",
      "reward: -6.71484375\n",
      "reward: -8.1072998046875\n",
      "reward: -4.1237640380859375\n",
      "reward: 4.62957763671875\n",
      "reward: 6.6436767578125\n",
      "reward: 10.976959228515625\n",
      "reward: 4.3193206787109375\n",
      "reward: -2.687835693359375\n",
      "reward: -2.9630889892578125\n",
      "reward: 4.1902313232421875\n",
      "reward: 9.706695556640625\n",
      "reward: 0.834686279296875\n",
      "reward: -8.876296997070312\n",
      "reward: -3.04840087890625\n",
      "reward: 1.1937713623046875\n",
      "reward: -5.3840179443359375\n",
      "reward: -9.883895874023438\n",
      "reward: -14.432510375976562\n",
      "reward: -9.983261108398438\n",
      "reward: -5.157501220703125\n",
      "reward: -3.478912353515625\n",
      "reward: -2.157623291015625\n",
      "reward: -0.3101043701171875\n",
      "reward: 0.0453033447265625\n",
      "reward: 0.1690216064453125\n",
      "reward: 0.0334625244140625\n",
      "reward: 0.102691650390625\n",
      "reward: 0.3271026611328125\n",
      "reward: 0.1673126220703125\n",
      "reward: 0.697021484375\n",
      "reward: 0.836151123046875\n",
      "reward: -6.23199462890625\n",
      "reward: -7.031646728515625\n",
      "reward: -2.16986083984375\n",
      "reward: -1.4636993408203125\n",
      "reward: -7.621978759765625\n",
      "reward: -8.625320434570312\n",
      "reward: -5.8180389404296875\n",
      "reward: -3.9244842529296875\n",
      "reward: -5.733856201171875\n",
      "reward: 0.0\n",
      "reward: 6.634521484375\n",
      "reward: 7.9590301513671875\n",
      "reward: 5.368377685546875\n",
      "reward: -3.1619873046875\n",
      "reward: -4.8729095458984375\n",
      "reward: -2.8553009033203125\n",
      "reward: -1.8824462890625\n",
      "reward: -1.2469482421875\n",
      "reward: -0.7975616455078125\n",
      "reward: -0.3329620361328125\n",
      "reward: -0.025177001953125\n",
      "reward: 0.0409088134765625\n",
      "reward: -4.7905120849609375\n",
      "reward: -0.0438079833984375\n",
      "reward: -0.0090179443359375\n",
      "reward: -0.0047607421875\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 2.8751373291015625\n",
      "reward: 1.5178070068359375\n",
      "reward: -3.9641876220703125\n",
      "reward: 0.491912841796875\n",
      "reward: 2.0540771484375\n",
      "reward: 1.385406494140625\n",
      "reward: 0.9344024658203125\n",
      "reward: 0.630157470703125\n",
      "reward: -0.3971405029296875\n",
      "reward: -5.4448089599609375\n",
      "reward: -0.082672119140625\n",
      "reward: 0.0\n",
      "reward: 0.821929931640625\n",
      "reward: -0.8014678955078125\n",
      "reward: 6.78277587890625\n",
      "reward: 7.3148040771484375\n",
      "reward: 3.876190185546875\n",
      "reward: 1.095001220703125\n",
      "reward: 0.713958740234375\n",
      "reward: -6.3014984130859375\n",
      "reward: -7.8125762939453125\n",
      "reward: -5.26983642578125\n",
      "reward: -0.399322509765625\n",
      "reward: 6.634521484375\n",
      "reward: 7.9590301513671875\n",
      "reward: 5.368377685546875\n",
      "reward: -0.048248291015625\n",
      "reward: -1.4949188232421875\n",
      "reward: -1.0084381103515625\n",
      "reward: -0.6803131103515625\n",
      "reward: -5.547119140625\n",
      "reward: -1.3255462646484375\n",
      "reward: -2.9186248779296875\n",
      "reward: -2.9853363037109375\n",
      "reward: -2.0137786865234375\n",
      "reward: -1.3584747314453125\n",
      "reward: -0.5971221923828125\n",
      "reward: 5.2081451416015625\n",
      "reward: 11.456100463867188\n",
      "reward: 15.110092163085938\n",
      "reward: 12.632675170898438\n",
      "reward: 13.096572875976562\n",
      "reward: 6.6607666015625\n",
      "reward: 2.0897369384765625\n",
      "reward: 1.409423828125\n",
      "reward: 7.33709716796875\n",
      "reward: 1.9161376953125\n",
      "reward: -2.0614013671875\n",
      "reward: -1.3905029296875\n",
      "reward: 0.7425689697265625\n",
      "reward: -3.928070068359375\n",
      "reward: -10.152816772460938\n",
      "reward: -3.25030517578125\n",
      "reward: -5.3430023193359375\n",
      "reward: -3.7194976806640625\n",
      "reward: -6.3216400146484375\n",
      "reward: -7.3787689208984375\n",
      "reward: -10.840133666992188\n",
      "reward: -3.5706329345703125\n",
      "reward: 1.72296142578125\n",
      "reward: 0.5517425537109375\n",
      "reward: -0.0596466064453125\n",
      "reward: -0.0403594970703125\n",
      "reward: 1.6533203125\n",
      "reward: -4.6369476318359375\n",
      "reward: -6.6118927001953125\n",
      "reward: -10.985549926757812\n",
      "reward: 3.95904541015625\n",
      "reward: 7.1619720458984375\n",
      "reward: 10.723068237304688\n",
      "reward: 10.716827392578125\n",
      "reward: 7.22857666015625\n",
      "reward: 5.655181884765625\n",
      "reward: 2.7225341796875\n",
      "reward: 1.307525634765625\n",
      "reward: 1.2720489501953125\n",
      "reward: 2.0930938720703125\n",
      "reward: 7.5301513671875\n",
      "reward: 7.8965606689453125\n",
      "reward: -2.557342529296875\n",
      "reward: -11.196395874023438\n",
      "reward: -10.626937866210938\n",
      "reward: -7.1681060791015625\n",
      "reward: -3.2766265869140625\n",
      "reward: -1.3917999267578125\n",
      "reward: 5.8568572998046875\n",
      "reward: 8.216171264648438\n",
      "reward: 3.4612579345703125\n",
      "reward: 1.717315673828125\n",
      "reward: -5.6688690185546875\n",
      "reward: -7.4090423583984375\n",
      "Result: 23.526016235351562\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: -7.11376953125\n",
      "reward: -8.534164428710938\n",
      "reward: -0.300262451171875\n",
      "reward: 0.8358306884765625\n",
      "reward: 6.9889373779296875\n",
      "reward: 4.9228363037109375\n",
      "reward: 1.9546051025390625\n",
      "reward: 0.8792266845703125\n",
      "reward: 0.592987060546875\n",
      "reward: 0.399871826171875\n",
      "reward: 6.6233978271484375\n",
      "reward: 8.021224975585938\n",
      "reward: -1.6103973388671875\n",
      "reward: -14.626266479492188\n",
      "reward: -11.944793701171875\n",
      "reward: -3.0271148681640625\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: -0.0050201416015625\n",
      "reward: -0.0493621826171875\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 2.2341156005859375\n",
      "reward: 1.1354522705078125\n",
      "reward: -3.0471343994140625\n",
      "reward: 0.0\n",
      "reward: -0.3252105712890625\n",
      "reward: 6.948944091796875\n",
      "reward: 1.3871307373046875\n",
      "reward: -2.713653564453125\n",
      "reward: -1.8304595947265625\n",
      "reward: 5.7142486572265625\n",
      "reward: 6.01507568359375\n",
      "reward: 6.7079010009765625\n",
      "reward: 7.3665771484375\n",
      "reward: 4.968780517578125\n",
      "reward: 3.3514556884765625\n",
      "reward: 4.5216064453125\n",
      "reward: 1.9759521484375\n",
      "reward: 2.406402587890625\n",
      "reward: 5.0715484619140625\n",
      "reward: 4.240478515625\n",
      "reward: 3.8170623779296875\n",
      "reward: 5.467498779296875\n",
      "reward: 4.654052734375\n",
      "reward: 0.6584320068359375\n",
      "reward: -3.0768890380859375\n",
      "reward: -3.569000244140625\n",
      "reward: -2.4074249267578125\n",
      "reward: -5.0264434814453125\n",
      "reward: -7.39752197265625\n",
      "reward: -6.0677642822265625\n",
      "reward: -7.0020904541015625\n",
      "reward: -2.8165130615234375\n",
      "reward: -6.2783966064453125\n",
      "reward: -2.239898681640625\n",
      "reward: -0.5782012939453125\n",
      "reward: -0.3900909423828125\n",
      "reward: -0.2632293701171875\n",
      "reward: 6.287506103515625\n",
      "reward: 7.6360015869140625\n",
      "reward: 11.615676879882812\n",
      "reward: 11.229949951171875\n",
      "reward: 14.354415893554688\n",
      "reward: 5.5456695556640625\n",
      "reward: -5.239410400390625\n",
      "reward: -7.2062530517578125\n",
      "reward: -4.86083984375\n",
      "reward: -3.2788238525390625\n",
      "reward: -2.2117462158203125\n",
      "reward: -2.1283416748046875\n",
      "reward: 2.9270782470703125\n",
      "reward: -1.113983154296875\n",
      "reward: 2.017730712890625\n",
      "reward: -1.45794677734375\n",
      "reward: 1.835113525390625\n",
      "reward: -1.5811309814453125\n",
      "reward: -7.9429779052734375\n",
      "reward: -5.9100189208984375\n",
      "reward: -2.90826416015625\n",
      "reward: -1.9617919921875\n",
      "reward: -1.323333740234375\n",
      "reward: -7.6033935546875\n",
      "reward: -8.65277099609375\n",
      "reward: -5.836517333984375\n",
      "reward: -3.9369354248046875\n",
      "reward: 1.64739990234375\n",
      "reward: 3.2419586181640625\n",
      "reward: 7.8468170166015625\n",
      "reward: 4.81378173828125\n",
      "reward: 1.3682708740234375\n",
      "reward: 5.225799560546875\n",
      "reward: 3.2534942626953125\n",
      "reward: -3.9311981201171875\n",
      "reward: -5.5058746337890625\n",
      "reward: 1.215911865234375\n",
      "reward: -4.023651123046875\n",
      "reward: -10.208938598632812\n",
      "reward: -14.417221069335938\n",
      "reward: 0.67657470703125\n",
      "reward: -0.28143310546875\n",
      "reward: -2.756744384765625\n",
      "reward: 3.0282440185546875\n",
      "reward: -0.2786407470703125\n",
      "reward: -2.7548828125\n",
      "reward: -0.0602569580078125\n",
      "reward: 0.0\n",
      "reward: -0.000457763671875\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 4.26708984375\n",
      "reward: -0.4316253662109375\n",
      "reward: 1.9151611328125\n",
      "reward: -1.4705657958984375\n",
      "reward: -4.27960205078125\n",
      "reward: 0.0\n",
      "reward: 5.5503387451171875\n",
      "reward: 6.3438262939453125\n",
      "reward: -0.1219940185546875\n",
      "reward: -5.178375244140625\n",
      "reward: -1.5606536865234375\n",
      "reward: 0.8284912109375\n",
      "reward: 6.1091156005859375\n",
      "reward: 12.585708618164062\n",
      "reward: 14.9698486328125\n",
      "reward: 7.154388427734375\n",
      "reward: -1.2278594970703125\n",
      "reward: -4.0072479248046875\n",
      "reward: -6.285614013671875\n",
      "reward: -6.1212158203125\n",
      "reward: -6.84332275390625\n",
      "reward: -13.509231567382812\n",
      "reward: -8.67864990234375\n",
      "reward: 6.46514892578125\n",
      "reward: 1.2905426025390625\n",
      "reward: -2.5247802734375\n",
      "reward: -1.703125\n",
      "reward: -1.14886474609375\n",
      "reward: 2.068817138671875\n",
      "reward: 2.88873291015625\n",
      "reward: 1.151947021484375\n",
      "reward: 2.4444427490234375\n",
      "reward: -3.9661407470703125\n",
      "reward: -6.924163818359375\n",
      "reward: 6.7797088623046875\n",
      "reward: 8.133193969726562\n",
      "reward: 5.4858551025390625\n",
      "reward: 6.543975830078125\n",
      "reward: 5.9073028564453125\n",
      "reward: -2.069000244140625\n",
      "reward: -4.3531951904296875\n",
      "reward: -6.0779876708984375\n",
      "reward: 0.072265625\n",
      "reward: 3.2275238037109375\n",
      "reward: 2.1768951416015625\n",
      "reward: 5.4833221435546875\n",
      "reward: 1.6296234130859375\n",
      "reward: -1.1418304443359375\n",
      "reward: -4.2897491455078125\n",
      "reward: -8.155426025390625\n",
      "reward: -10.677200317382812\n",
      "reward: -8.679931640625\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 4.8877410888671875\n",
      "reward: 5.863494873046875\n",
      "reward: 3.95489501953125\n",
      "reward: 8.102325439453125\n",
      "reward: 12.622024536132812\n",
      "reward: 15.076309204101562\n",
      "reward: 8.125625610351562\n",
      "reward: 7.5240631103515625\n",
      "reward: 12.769485473632812\n",
      "reward: 16.90191650390625\n",
      "reward: 14.254547119140625\n",
      "reward: 14.50262451171875\n",
      "reward: 12.266189575195312\n",
      "reward: -2.7049713134765625\n",
      "reward: 0.977020263671875\n",
      "reward: 7.8408203125\n",
      "reward: 7.5294952392578125\n",
      "reward: 0.8113555908203125\n",
      "reward: -1.6937255859375\n",
      "reward: -1.2444305419921875\n",
      "reward: -1.091217041015625\n",
      "reward: 2.8462982177734375\n",
      "reward: 3.801055908203125\n",
      "reward: 2.5637664794921875\n",
      "reward: -1.303741455078125\n",
      "reward: 0.5605621337890625\n",
      "reward: 1.9705963134765625\n",
      "reward: -5.3105926513671875\n",
      "reward: -0.1777801513671875\n",
      "reward: 3.4039306640625\n",
      "reward: 2.2958526611328125\n",
      "reward: 1.548492431640625\n",
      "reward: 1.0443878173828125\n",
      "reward: 0.7043914794921875\n",
      "reward: 0.475067138671875\n",
      "reward: 0.94793701171875\n",
      "reward: -6.609344482421875\n",
      "reward: -8.312911987304688\n",
      "reward: -7.0626678466796875\n",
      "reward: -5.4308319091796875\n",
      "reward: -2.923431396484375\n",
      "reward: -2.270111083984375\n",
      "reward: -1.8609771728515625\n",
      "reward: -1.2553558349609375\n",
      "reward: -2.5012359619140625\n",
      "reward: -2.5559539794921875\n",
      "reward: -0.0699310302734375\n",
      "reward: 2.4756317138671875\n",
      "reward: -3.42498779296875\n",
      "reward: -6.8019866943359375\n",
      "reward: -7.111297607421875\n",
      "reward: 0.655731201171875\n",
      "reward: 1.1035308837890625\n",
      "reward: 0.7442626953125\n",
      "reward: -6.8553924560546875\n",
      "reward: -11.248794555664062\n",
      "reward: -10.615798950195312\n",
      "reward: -2.3771820068359375\n",
      "reward: -0.922882080078125\n",
      "reward: -4.557952880859375\n",
      "reward: -4.4632568359375\n",
      "reward: 3.5431671142578125\n",
      "reward: 11.608489990234375\n",
      "reward: 13.113250732421875\n",
      "reward: 16.742156982421875\n",
      "reward: 17.07379150390625\n",
      "reward: 10.444961547851562\n",
      "reward: 5.8603973388671875\n",
      "reward: 3.952850341796875\n",
      "reward: 9.779891967773438\n",
      "reward: 17.446060180664062\n",
      "reward: 15.503280639648438\n",
      "reward: 11.793930053710938\n",
      "reward: 8.319869995117188\n",
      "reward: 5.6117401123046875\n",
      "reward: 3.7850799560546875\n",
      "reward: 2.5529937744140625\n",
      "reward: 2.5578155517578125\n",
      "reward: 2.164154052734375\n",
      "reward: 1.45965576171875\n",
      "reward: 0.984466552734375\n",
      "reward: 2.097442626953125\n",
      "reward: 6.96673583984375\n",
      "reward: 9.010101318359375\n",
      "reward: 11.878402709960938\n",
      "reward: 18.6102294921875\n",
      "reward: 16.201934814453125\n",
      "reward: 17.877288818359375\n",
      "reward: 14.252182006835938\n",
      "reward: 8.848968505859375\n",
      "reward: -1.7234344482421875\n",
      "reward: -11.881362915039062\n",
      "reward: -10.557113647460938\n",
      "reward: -6.791534423828125\n",
      "reward: -4.5811004638671875\n",
      "reward: -1.834381103515625\n",
      "reward: 6.5549163818359375\n",
      "reward: 8.09332275390625\n",
      "reward: 5.458984375\n",
      "reward: 3.6820220947265625\n",
      "reward: 2.4835052490234375\n",
      "reward: 1.675048828125\n",
      "reward: -99.67459106445312\n",
      "Result: 213.39834594726562\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: -0.209564208984375\n",
      "reward: -0.251495361328125\n",
      "reward: -5.9635467529296875\n",
      "reward: -7.42181396484375\n",
      "reward: -2.112762451171875\n",
      "reward: 0.209320068359375\n",
      "reward: 0.251007080078125\n",
      "reward: 0.169219970703125\n",
      "reward: -0.64703369140625\n",
      "reward: 0.413482666015625\n",
      "reward: -0.4169158935546875\n",
      "reward: -0.0184783935546875\n",
      "reward: 0.0\n",
      "reward: 0.1773834228515625\n",
      "reward: 0.685821533203125\n",
      "reward: 1.083648681640625\n",
      "reward: 0.23736572265625\n",
      "reward: 0.3563690185546875\n",
      "reward: 0.4574127197265625\n",
      "reward: 1.5363006591796875\n",
      "reward: 1.6450653076171875\n",
      "reward: 1.1095428466796875\n",
      "reward: 0.7482757568359375\n",
      "reward: -6.5247955322265625\n",
      "reward: -1.4409637451171875\n",
      "reward: -0.0484619140625\n",
      "reward: 0.0\n",
      "reward: 7.0834503173828125\n",
      "reward: 9.055618286132812\n",
      "reward: 5.8467254638671875\n",
      "reward: 3.943634033203125\n",
      "reward: -4.4503631591796875\n",
      "reward: -6.7358245849609375\n",
      "reward: -11.572952270507812\n",
      "reward: -3.1705474853515625\n",
      "reward: 7.029266357421875\n",
      "reward: 1.4031982421875\n",
      "reward: -2.74505615234375\n",
      "reward: -5.3071136474609375\n",
      "reward: 2.548492431640625\n",
      "reward: 4.9091796875\n",
      "reward: 3.3111724853515625\n",
      "reward: 2.2333221435546875\n",
      "reward: -5.6039886474609375\n",
      "reward: -7.5139923095703125\n",
      "reward: -0.2429962158203125\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.3177642822265625\n",
      "reward: 0.3945770263671875\n",
      "reward: 0.266082763671875\n",
      "reward: 0.4185791015625\n",
      "reward: 0.9206695556640625\n",
      "reward: -2.3402557373046875\n",
      "reward: 0.32366943359375\n",
      "reward: 0.3971710205078125\n",
      "reward: -0.1510162353515625\n",
      "reward: -0.5857086181640625\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 5.2169342041015625\n",
      "reward: -0.4333343505859375\n",
      "reward: -3.8709716796875\n",
      "reward: -0.884796142578125\n",
      "reward: -0.0117034912109375\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: -0.0092010498046875\n",
      "reward: 5.6165618896484375\n",
      "reward: 3.70745849609375\n",
      "reward: 2.5006256103515625\n",
      "reward: 1.6866302490234375\n",
      "reward: 6.640716552734375\n",
      "reward: 6.595977783203125\n",
      "reward: 4.30242919921875\n",
      "reward: 10.764785766601562\n",
      "reward: 10.821182250976562\n",
      "reward: 7.2989501953125\n",
      "reward: 4.9231719970703125\n",
      "reward: 3.320648193359375\n",
      "reward: 5.82220458984375\n",
      "reward: 5.769439697265625\n",
      "reward: 7.57330322265625\n",
      "reward: 8.503326416015625\n",
      "reward: 5.7354888916015625\n",
      "reward: -1.2495880126953125\n",
      "reward: -3.5307159423828125\n",
      "reward: -0.89715576171875\n",
      "reward: -6.3205108642578125\n",
      "reward: -13.566787719726562\n",
      "reward: -12.354507446289062\n",
      "reward: -14.011810302734375\n",
      "reward: -12.433258056640625\n",
      "reward: -5.3348236083984375\n",
      "reward: -2.8372955322265625\n",
      "reward: 3.5665130615234375\n",
      "reward: 0.9719696044921875\n",
      "reward: -1.4766693115234375\n",
      "reward: 3.9255523681640625\n",
      "reward: 5.1527252197265625\n",
      "reward: -1.65167236328125\n",
      "reward: -9.551300048828125\n",
      "reward: -8.792510986328125\n",
      "reward: -0.812713623046875\n",
      "reward: -2.9321136474609375\n",
      "reward: -5.29205322265625\n",
      "reward: -10.709396362304688\n",
      "reward: 1.7775726318359375\n",
      "reward: -0.294677734375\n",
      "reward: -101.39350891113281\n",
      "Result: -114.45176696777344\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: -7.11376953125\n",
      "reward: -8.534164428710938\n",
      "reward: -0.327667236328125\n",
      "reward: 0.0\n",
      "reward: 6.1460723876953125\n",
      "reward: 7.397796630859375\n",
      "reward: 4.9898681640625\n",
      "reward: 9.770782470703125\n",
      "reward: 15.79327392578125\n",
      "reward: 6.6524658203125\n",
      "reward: 0.7798614501953125\n",
      "reward: -1.07110595703125\n",
      "reward: -2.8664703369140625\n",
      "reward: -13.064682006835938\n",
      "reward: -1.6086883544921875\n",
      "reward: 0.967498779296875\n",
      "reward: -7.1204833984375\n",
      "reward: -15.325332641601562\n",
      "reward: -11.44158935546875\n",
      "reward: 1.6591796875\n",
      "reward: -1.6591796875\n",
      "reward: 0.0\n",
      "reward: -0.000732421875\n",
      "reward: 6.626190185546875\n",
      "reward: 8.040359497070312\n",
      "reward: 12.058242797851562\n",
      "reward: 11.617645263671875\n",
      "reward: 1.2010345458984375\n",
      "reward: -0.2165985107421875\n",
      "reward: 1.1363677978515625\n",
      "reward: 0.764190673828125\n",
      "reward: -5.392120361328125\n",
      "reward: -0.3709716796875\n",
      "reward: -3.2283782958984375\n",
      "reward: -11.630767822265625\n",
      "reward: -1.0734710693359375\n",
      "reward: 0.980712890625\n",
      "reward: -1.9443511962890625\n",
      "reward: -2.022674560546875\n",
      "reward: 5.188690185546875\n",
      "reward: 4.2889404296875\n",
      "reward: 8.05328369140625\n",
      "reward: 11.913375854492188\n",
      "reward: 10.035125732421875\n",
      "reward: 7.3028717041015625\n",
      "reward: -4.0391082763671875\n",
      "reward: -6.317962646484375\n",
      "reward: -4.2617034912109375\n",
      "reward: -8.599166870117188\n",
      "reward: -8.348220825195312\n",
      "reward: -8.195068359375\n",
      "reward: -5.527801513671875\n",
      "reward: -9.756942749023438\n",
      "reward: -8.23968505859375\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: -0.012786865234375\n",
      "reward: -0.0311431884765625\n",
      "reward: 0.0\n",
      "reward: 3.3600616455078125\n",
      "reward: 4.0183868408203125\n",
      "reward: 9.263504028320312\n",
      "reward: 9.689590454101562\n",
      "reward: 6.5356903076171875\n",
      "reward: -2.4345550537109375\n",
      "reward: -12.078536987304688\n",
      "reward: -11.740646362304688\n",
      "reward: -6.6254425048828125\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 6.1650543212890625\n",
      "reward: 11.380142211914062\n",
      "reward: 5.0883026123046875\n",
      "reward: 3.4320526123046875\n",
      "reward: 2.314849853515625\n",
      "reward: 7.88116455078125\n",
      "reward: 8.570892333984375\n",
      "reward: 5.78106689453125\n",
      "reward: 3.8993072509765625\n",
      "reward: -3.5351715087890625\n",
      "reward: -5.622222900390625\n",
      "reward: -3.7923583984375\n",
      "reward: 3.2547607421875\n",
      "reward: 9.180145263671875\n",
      "reward: 14.069915771484375\n",
      "reward: 12.542831420898438\n",
      "reward: 8.460281372070312\n",
      "reward: -0.5647735595703125\n",
      "reward: -4.274688720703125\n",
      "reward: -10.638595581054688\n",
      "reward: -10.469223022460938\n",
      "reward: -7.0617523193359375\n",
      "reward: -4.76336669921875\n",
      "reward: -6.972198486328125\n",
      "reward: -6.677001953125\n",
      "reward: -10.875091552734375\n",
      "reward: -13.714263916015625\n",
      "reward: -10.843276977539062\n",
      "reward: -0.3685302734375\n",
      "reward: 0.1995086669921875\n",
      "reward: -1.4384918212890625\n",
      "reward: -0.6111907958984375\n",
      "reward: 6.2359771728515625\n",
      "reward: 9.831497192382812\n",
      "reward: 14.5570068359375\n",
      "reward: 19.938079833984375\n",
      "reward: 16.932693481445312\n",
      "reward: 11.4212646484375\n",
      "reward: 1.4324951171875\n",
      "reward: -5.5463714599609375\n",
      "reward: -11.703018188476562\n",
      "reward: -17.458465576171875\n",
      "reward: -21.340606689453125\n",
      "reward: -17.68798828125\n",
      "reward: -6.5508270263671875\n",
      "reward: 6.271026611328125\n",
      "reward: 13.794052124023438\n",
      "reward: 12.59735107421875\n",
      "reward: 2.6839752197265625\n",
      "reward: -1.2423858642578125\n",
      "reward: -4.7704010009765625\n",
      "reward: -5.282806396484375\n",
      "reward: -3.5634002685546875\n",
      "reward: -2.4036865234375\n",
      "reward: -1.6214141845703125\n",
      "reward: -6.361175537109375\n",
      "reward: -7.056915283203125\n",
      "reward: -3.0779571533203125\n",
      "reward: 5.684295654296875\n",
      "reward: 6.8190765380859375\n",
      "reward: 4.5995025634765625\n",
      "reward: 8.786697387695312\n",
      "reward: 9.621200561523438\n",
      "reward: 7.8071746826171875\n",
      "reward: 4.665252685546875\n",
      "reward: 2.9827728271484375\n",
      "reward: 5.4141998291015625\n",
      "reward: 5.438568115234375\n",
      "reward: 5.3806304931640625\n",
      "reward: 6.27471923828125\n",
      "reward: 11.923492431640625\n",
      "reward: 14.027206420898438\n",
      "reward: -1.4010162353515625\n",
      "reward: -2.337799072265625\n",
      "reward: -8.1302490234375\n",
      "reward: -6.273529052734375\n",
      "reward: -0.19970703125\n",
      "reward: -1.4772186279296875\n",
      "reward: -8.9425048828125\n",
      "reward: -9.473358154296875\n",
      "reward: -8.251876831054688\n",
      "reward: -6.543853759765625\n",
      "reward: -3.3707275390625\n",
      "reward: -1.725830078125\n",
      "reward: -2.207611083984375\n",
      "reward: 3.0147705078125\n",
      "reward: 9.891342163085938\n",
      "reward: 9.917312622070312\n",
      "reward: 6.6892852783203125\n",
      "reward: 10.274917602539062\n",
      "reward: 15.989227294921875\n",
      "reward: 13.602798461914062\n",
      "reward: 9.25946044921875\n",
      "reward: 6.35552978515625\n",
      "reward: 4.286773681640625\n",
      "reward: 2.268951416015625\n",
      "reward: 0.932037353515625\n",
      "reward: -6.5188140869140625\n",
      "reward: -8.117019653320312\n",
      "reward: -7.256988525390625\n",
      "reward: -5.4340972900390625\n",
      "reward: -3.6655120849609375\n",
      "reward: -9.501983642578125\n",
      "reward: -10.10076904296875\n",
      "reward: -13.656051635742188\n",
      "reward: -8.49383544921875\n",
      "reward: 4.755462646484375\n",
      "reward: 8.385025024414062\n",
      "reward: 6.63336181640625\n",
      "reward: 4.4742279052734375\n",
      "reward: 5.6697998046875\n",
      "reward: 4.534332275390625\n",
      "reward: -3.0557861328125\n",
      "reward: -2.5523529052734375\n",
      "reward: 2.28515625\n",
      "reward: 2.8822174072265625\n",
      "reward: 0.435394287109375\n",
      "reward: -6.0874481201171875\n",
      "reward: -5.4989471435546875\n",
      "reward: -1.8475494384765625\n",
      "reward: 6.5740203857421875\n",
      "reward: 9.889236450195312\n",
      "reward: 7.6479949951171875\n",
      "reward: 5.33551025390625\n",
      "reward: 4.039520263671875\n",
      "reward: -3.7747344970703125\n",
      "reward: 1.0125732421875\n",
      "reward: 4.374267578125\n",
      "reward: 2.9503936767578125\n",
      "reward: 1.9899444580078125\n",
      "reward: 5.864654541015625\n",
      "reward: 2.7488250732421875\n",
      "reward: -1.47943115234375\n",
      "reward: -1.9076690673828125\n",
      "reward: -7.2898101806640625\n",
      "reward: -14.072509765625\n",
      "reward: -12.644638061523438\n",
      "reward: -2.1728515625\n",
      "reward: 8.228408813476562\n",
      "reward: 11.393142700195312\n",
      "reward: 3.433013916015625\n",
      "reward: -3.50738525390625\n",
      "reward: -1.8233642578125\n",
      "reward: 0.55487060546875\n",
      "reward: 6.730438232421875\n",
      "reward: 7.8776092529296875\n",
      "reward: 3.5478515625\n",
      "reward: 7.9242706298828125\n",
      "reward: 3.543609619140625\n",
      "reward: 5.22637939453125\n",
      "reward: 6.6931610107421875\n",
      "reward: 4.514556884765625\n",
      "reward: -3.6974945068359375\n",
      "reward: 0.707550048828125\n",
      "reward: 2.459991455078125\n",
      "reward: -0.3931121826171875\n",
      "reward: -7.568145751953125\n",
      "reward: -7.5211029052734375\n",
      "reward: -5.607330322265625\n",
      "reward: -4.3729400634765625\n",
      "reward: -100.10012817382812\n",
      "Result: 11.204055786132812\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: -5.9390411376953125\n",
      "reward: -9.94244384765625\n",
      "reward: -0.1056365966796875\n",
      "reward: 3.7546844482421875\n",
      "reward: 0.7494964599609375\n",
      "reward: -1.4663238525390625\n",
      "reward: -3.03216552734375\n",
      "reward: 4.424652099609375\n",
      "reward: 3.1388092041015625\n",
      "reward: 0.4004058837890625\n",
      "reward: 0.2699737548828125\n",
      "reward: -4.884368896484375\n",
      "reward: -3.274139404296875\n",
      "reward: -0.0748443603515625\n",
      "reward: -0.0044403076171875\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: -0.0016937255859375\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 3.5472259521484375\n",
      "reward: 9.80706787109375\n",
      "reward: 9.399276733398438\n",
      "reward: 10.095428466796875\n",
      "reward: 10.105804443359375\n",
      "reward: 13.647354125976562\n",
      "reward: 3.350250244140625\n",
      "reward: -7.6510162353515625\n",
      "reward: -0.278167724609375\n",
      "reward: -2.672271728515625\n",
      "reward: -4.49237060546875\n",
      "reward: 1.06707763671875\n",
      "reward: 2.871307373046875\n",
      "reward: 1.57330322265625\n",
      "reward: -4.0452880859375\n",
      "reward: -5.1896820068359375\n",
      "reward: -3.5006561279296875\n",
      "reward: -2.361328125\n",
      "reward: -1.5928192138671875\n",
      "reward: 5.3931121826171875\n",
      "reward: 0.5663299560546875\n",
      "reward: -100.55648803710938\n",
      "Result: -76.90362548828125\n",
      "reward: 0.0\n",
      "reward: 0.78125\n",
      "reward: 2.4580535888671875\n",
      "reward: 1.657928466796875\n",
      "reward: 0.2822265625\n",
      "reward: 6.8106536865234375\n",
      "reward: 7.4649810791015625\n",
      "reward: -2.4634857177734375\n",
      "reward: -5.096435546875\n",
      "reward: -2.888824462890625\n",
      "reward: -1.948638916015625\n",
      "reward: -1.314453125\n",
      "reward: -0.8866729736328125\n",
      "reward: 1.85955810546875\n",
      "reward: 0.087005615234375\n",
      "reward: -1.232147216796875\n",
      "reward: -7.46630859375\n",
      "reward: -1.8856353759765625\n",
      "reward: 2.2122650146484375\n",
      "reward: 8.391265869140625\n",
      "reward: 1.716094970703125\n",
      "reward: -1.3089141845703125\n",
      "reward: -0.01165771484375\n",
      "reward: -6.9072265625\n",
      "reward: -8.282211303710938\n",
      "reward: 1.463043212890625\n",
      "reward: 11.746231079101562\n",
      "reward: 4.57049560546875\n",
      "reward: 6.434967041015625\n",
      "reward: 8.047592163085938\n",
      "reward: -1.6856536865234375\n",
      "reward: -4.8728179931640625\n",
      "reward: -3.2868804931640625\n",
      "reward: -3.0283355712890625\n",
      "reward: 5.8185577392578125\n",
      "reward: 7.0399322509765625\n",
      "reward: 11.267349243164062\n",
      "reward: 12.635757446289062\n",
      "reward: 9.394241333007812\n",
      "reward: -0.723114013671875\n",
      "reward: -4.19512939453125\n",
      "reward: -2.2354583740234375\n",
      "reward: -0.0274658203125\n",
      "reward: 0.0\n",
      "reward: -6.712554931640625\n",
      "reward: -14.765365600585938\n",
      "reward: -6.7722015380859375\n",
      "reward: -1.0431671142578125\n",
      "reward: 1.5523681640625\n",
      "reward: -4.4807891845703125\n",
      "reward: -0.2136383056640625\n",
      "reward: 2.490447998046875\n",
      "reward: 1.6797332763671875\n",
      "reward: 1.1329345703125\n",
      "reward: 0.764068603515625\n",
      "reward: -2.69940185546875\n",
      "reward: -3.5090179443359375\n",
      "reward: -2.36700439453125\n",
      "reward: -5.5247955322265625\n",
      "reward: -2.026092529296875\n",
      "reward: -6.205322265625\n",
      "reward: -3.312042236328125\n",
      "reward: 5.6446380615234375\n",
      "reward: 10.78955078125\n",
      "reward: 5.7967987060546875\n",
      "reward: 1.113677978515625\n",
      "reward: 0.7510986328125\n",
      "reward: -4.4010162353515625\n",
      "reward: -5.4811553955078125\n",
      "reward: -3.6972503662109375\n",
      "reward: -6.883392333984375\n",
      "reward: -11.916671752929688\n",
      "reward: -6.079925537109375\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 3.2145233154296875\n",
      "reward: -2.36859130859375\n",
      "reward: -0.3043212890625\n",
      "reward: -0.547088623046875\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 3.927947998046875\n",
      "reward: 10.528182983398438\n",
      "reward: 4.33935546875\n",
      "reward: -0.1266021728515625\n",
      "reward: -3.2147064208984375\n",
      "reward: -3.8566436767578125\n",
      "reward: -2.601470947265625\n",
      "reward: -1.7548065185546875\n",
      "reward: -1.183746337890625\n",
      "reward: -0.7985382080078125\n",
      "reward: -0.538726806640625\n",
      "reward: 6.720001220703125\n",
      "reward: 6.4302978515625\n",
      "reward: 1.757568359375\n",
      "reward: 1.5121307373046875\n",
      "reward: -6.06378173828125\n",
      "reward: -7.1428985595703125\n",
      "reward: -6.5166778564453125\n",
      "reward: -1.3640899658203125\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.6186676025390625\n",
      "reward: -0.5488128662109375\n",
      "reward: 4.287445068359375\n",
      "reward: 6.675872802734375\n",
      "reward: 4.7122802734375\n",
      "reward: 3.28839111328125\n",
      "reward: 2.008453369140625\n",
      "reward: 1.03509521484375\n",
      "reward: 0.318084716796875\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: -5.81304931640625\n",
      "reward: -6.973724365234375\n",
      "reward: -4.7039794921875\n",
      "reward: -4.950469970703125\n",
      "reward: 0.0\n",
      "reward: 5.2671966552734375\n",
      "reward: 10.904937744140625\n",
      "reward: 9.763824462890625\n",
      "reward: 6.5857696533203125\n",
      "reward: -0.3634490966796875\n",
      "reward: 2.387908935546875\n",
      "reward: 9.253875732421875\n",
      "reward: 3.85150146484375\n",
      "reward: -5.418243408203125\n",
      "reward: -1.2653656005859375\n",
      "reward: 1.788818359375\n",
      "reward: 6.6912841796875\n",
      "reward: 6.0290985107421875\n",
      "reward: 4.066619873046875\n",
      "reward: 7.7812652587890625\n",
      "reward: 4.3445892333984375\n",
      "reward: 6.674530029296875\n",
      "reward: 1.3328704833984375\n",
      "reward: -2.253448486328125\n",
      "reward: 4.0390625\n",
      "reward: 5.6436309814453125\n",
      "reward: 8.612030029296875\n",
      "reward: 3.3004608154296875\n",
      "reward: -0.4163360595703125\n",
      "reward: -3.55401611328125\n",
      "reward: 2.69732666015625\n",
      "reward: -1.266510009765625\n",
      "reward: -3.696197509765625\n",
      "reward: 2.1582794189453125\n",
      "reward: -2.2860565185546875\n",
      "reward: 0.84735107421875\n",
      "reward: -1.555938720703125\n",
      "reward: -2.25262451171875\n",
      "reward: 2.607757568359375\n",
      "reward: -0.2011566162109375\n",
      "reward: -6.3660125732421875\n",
      "reward: 1.6358184814453125\n",
      "reward: 3.2706756591796875\n",
      "reward: -1.181060791015625\n",
      "reward: -2.575408935546875\n",
      "reward: -7.2963714599609375\n",
      "reward: -13.400070190429688\n",
      "reward: -16.085372924804688\n",
      "reward: -13.017410278320312\n",
      "reward: -13.812423706054688\n",
      "reward: -7.15380859375\n",
      "reward: -1.99700927734375\n",
      "reward: -5.602020263671875\n",
      "reward: -2.079833984375\n",
      "reward: 6.3302001953125\n",
      "reward: 7.189056396484375\n",
      "reward: -0.710174560546875\n",
      "reward: -3.3984222412109375\n",
      "reward: -8.2952880859375\n",
      "reward: -7.508148193359375\n",
      "reward: -0.05401611328125\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 6.0027618408203125\n",
      "reward: 7.2011566162109375\n",
      "reward: -1.145721435546875\n",
      "reward: -5.81060791015625\n",
      "reward: -6.194610595703125\n",
      "reward: 3.38690185546875\n",
      "reward: 2.7873992919921875\n",
      "reward: 7.23828125\n",
      "reward: 1.86376953125\n",
      "reward: -2.0809326171875\n",
      "reward: -6.1570892333984375\n",
      "reward: -4.1173095703125\n",
      "reward: 3.5789947509765625\n",
      "reward: 3.1560516357421875\n",
      "reward: -5.5908660888671875\n",
      "reward: -4.077239990234375\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 3.885406494140625\n",
      "reward: -1.1272430419921875\n",
      "reward: -2.219696044921875\n",
      "reward: -0.5695037841796875\n",
      "reward: 0.0\n",
      "reward: -0.010467529296875\n",
      "reward: 3.21905517578125\n",
      "reward: 3.8617095947265625\n",
      "reward: 2.495635986328125\n",
      "reward: 7.4101409912109375\n",
      "reward: 4.3560791015625\n",
      "reward: 0.9640350341796875\n",
      "reward: 0.650146484375\n",
      "reward: 5.335968017578125\n",
      "reward: 6.292327880859375\n",
      "reward: 4.24420166015625\n",
      "reward: 2.862640380859375\n",
      "reward: -5.128814697265625\n",
      "reward: -14.226409912109375\n",
      "reward: -12.467437744140625\n",
      "reward: -9.856109619140625\n",
      "reward: 7.0593719482421875\n",
      "reward: 1.40911865234375\n",
      "reward: -1.920989990234375\n",
      "reward: -0.6951751708984375\n",
      "reward: -0.3466796875\n",
      "reward: -0.128448486328125\n",
      "reward: -5.316680908203125\n",
      "reward: 0.0\n",
      "reward: -0.05462646484375\n",
      "reward: 0.0\n",
      "reward: -0.0272674560546875\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.6164703369140625\n",
      "reward: 7.8191070556640625\n",
      "reward: 8.365753173828125\n",
      "reward: 5.313079833984375\n",
      "reward: -3.526824951171875\n",
      "reward: -6.1130218505859375\n",
      "reward: -3.553802490234375\n",
      "reward: -1.3515167236328125\n",
      "reward: -0.653717041015625\n",
      "reward: -0.51873779296875\n",
      "reward: -0.4572601318359375\n",
      "reward: 0.3190765380859375\n",
      "reward: 0.4439849853515625\n",
      "reward: -0.4044189453125\n",
      "reward: -0.6025238037109375\n",
      "reward: 6.6764068603515625\n",
      "reward: 8.222732543945312\n",
      "reward: 5.5462799072265625\n",
      "reward: 2.2778472900390625\n",
      "reward: 0.7594757080078125\n",
      "reward: 0.512176513671875\n",
      "reward: 7.405487060546875\n",
      "reward: 8.702606201171875\n",
      "reward: 6.4949188232421875\n",
      "reward: 3.4217529296875\n",
      "reward: 2.7023773193359375\n",
      "reward: 2.2589111328125\n",
      "reward: 8.637298583984375\n",
      "reward: 9.561614990234375\n",
      "reward: 6.4493408203125\n",
      "reward: 4.350067138671875\n",
      "reward: 9.646453857421875\n",
      "reward: 9.964263916015625\n",
      "reward: 3.1139373779296875\n",
      "reward: 7.94140625\n",
      "reward: 8.478607177734375\n",
      "reward: 5.7188873291015625\n",
      "reward: 10.136856079101562\n",
      "reward: 7.676605224609375\n",
      "reward: 3.939361572265625\n",
      "reward: 5.6852874755859375\n",
      "reward: 5.424957275390625\n",
      "reward: 3.6591033935546875\n",
      "reward: 4.72412109375\n",
      "reward: -1.8519287109375\n",
      "reward: 2.8686370849609375\n",
      "reward: 3.2035064697265625\n",
      "reward: 7.6882476806640625\n",
      "reward: 8.652313232421875\n",
      "reward: 7.8234100341796875\n",
      "reward: 8.2144775390625\n",
      "reward: 5.5407257080078125\n",
      "reward: 4.50079345703125\n",
      "reward: -3.470977783203125\n",
      "reward: -12.059890747070312\n",
      "reward: -17.162277221679688\n",
      "reward: -20.195236206054688\n",
      "reward: -21.9981689453125\n",
      "reward: -17.689804077148438\n",
      "reward: -11.932144165039062\n",
      "reward: -8.04852294921875\n",
      "reward: -5.4289703369140625\n",
      "reward: -3.41986083984375\n",
      "reward: -8.417709350585938\n",
      "reward: -1.8238677978515625\n",
      "reward: 9.613662719726562\n",
      "reward: 9.823028564453125\n",
      "reward: 5.9304046630859375\n",
      "reward: 3.8192291259765625\n",
      "reward: 9.662063598632812\n",
      "reward: 10.25091552734375\n",
      "reward: -0.1959686279296875\n",
      "reward: -3.866180419921875\n",
      "reward: -1.985565185546875\n",
      "reward: -1.635040283203125\n",
      "reward: -1.429840087890625\n",
      "reward: -0.9645538330078125\n",
      "reward: -0.6507568359375\n",
      "reward: -0.439056396484375\n",
      "reward: 1.3579864501953125\n",
      "reward: 1.7845916748046875\n",
      "reward: 1.2035980224609375\n",
      "reward: 0.8117218017578125\n",
      "reward: -1.106964111328125\n",
      "reward: 5.2847900390625\n",
      "reward: 0.2877349853515625\n",
      "reward: -10.330184936523438\n",
      "reward: -10.591690063476562\n",
      "reward: -0.0842437744140625\n",
      "reward: 2.80535888671875\n",
      "reward: -0.2436676025390625\n",
      "reward: -0.5843353271484375\n",
      "reward: -1.9234619140625\n",
      "reward: -2.0755767822265625\n",
      "reward: -1.400146484375\n",
      "reward: -0.9445037841796875\n",
      "reward: -5.3038330078125\n",
      "reward: -0.92669677734375\n",
      "reward: -3.0130157470703125\n",
      "reward: 0.1104278564453125\n",
      "reward: 5.8157958984375\n",
      "reward: 0.92584228515625\n",
      "reward: -6.7855987548828125\n",
      "reward: -11.912384033203125\n",
      "reward: -10.560882568359375\n",
      "reward: -0.6826019287109375\n",
      "reward: -2.5764007568359375\n",
      "reward: 0.9004058837890625\n",
      "reward: 7.65667724609375\n",
      "reward: 3.20233154296875\n",
      "reward: 5.5462646484375\n",
      "reward: 10.79034423828125\n",
      "reward: 9.447845458984375\n",
      "reward: 2.9785003662109375\n",
      "reward: 0.03350830078125\n",
      "reward: -2.4120330810546875\n",
      "reward: -2.86102294921875\n",
      "reward: -1.9298858642578125\n",
      "reward: -1.3018341064453125\n",
      "reward: -0.878204345703125\n",
      "reward: -5.760223388671875\n",
      "reward: -11.245849609375\n",
      "reward: -10.025680541992188\n",
      "reward: -6.7173004150390625\n",
      "reward: -3.5063323974609375\n",
      "reward: -7.9214019775390625\n",
      "reward: -8.261001586914062\n",
      "reward: -0.762847900390625\n",
      "reward: 6.8204193115234375\n",
      "reward: 9.607192993164062\n",
      "reward: 8.50274658203125\n",
      "reward: 0.9255523681640625\n",
      "reward: 2.9079132080078125\n",
      "reward: 4.4869537353515625\n",
      "reward: 8.0546875\n",
      "reward: 3.0450286865234375\n",
      "reward: -5.615203857421875\n",
      "reward: -1.6188201904296875\n",
      "reward: 4.9790802001953125\n",
      "reward: 4.84783935546875\n",
      "reward: 3.26983642578125\n",
      "reward: -4.068267822265625\n",
      "reward: -2.8242340087890625\n",
      "reward: -3.43170166015625\n",
      "reward: -4.0030364990234375\n",
      "reward: -5.9149017333984375\n",
      "reward: -8.892623901367188\n",
      "reward: -7.2120513916015625\n",
      "reward: -5.589996337890625\n",
      "reward: -6.7127685546875\n",
      "reward: -5.5494537353515625\n",
      "reward: -2.089080810546875\n",
      "reward: 6.35986328125\n",
      "reward: 1.0128936767578125\n",
      "reward: -1.286346435546875\n",
      "reward: 1.1737213134765625\n",
      "reward: -5.071044921875\n",
      "reward: -4.98956298828125\n",
      "reward: 4.4414520263671875\n",
      "reward: 13.519699096679688\n",
      "reward: 14.39703369140625\n",
      "reward: 8.079315185546875\n",
      "reward: 4.7406463623046875\n",
      "reward: 3.8546905517578125\n",
      "reward: 0.945556640625\n",
      "reward: -0.2310943603515625\n",
      "reward: 1.4982147216796875\n",
      "reward: 1.879180908203125\n",
      "reward: 7.9043121337890625\n",
      "reward: 8.816802978515625\n",
      "reward: 4.2926483154296875\n",
      "reward: 2.0265960693359375\n",
      "reward: 7.9155120849609375\n",
      "reward: 11.573837280273438\n",
      "reward: 12.864898681640625\n",
      "reward: 9.965545654296875\n",
      "reward: 13.622177124023438\n",
      "reward: 12.81195068359375\n",
      "reward: 8.641754150390625\n",
      "reward: 12.911819458007812\n",
      "reward: 11.800888061523438\n",
      "reward: 2.342498779296875\n",
      "reward: -0.574371337890625\n",
      "reward: -0.6880950927734375\n",
      "reward: -0.4642181396484375\n",
      "reward: -0.3132171630859375\n",
      "reward: -7.2944183349609375\n",
      "reward: -15.722976684570312\n",
      "reward: -13.69744873046875\n",
      "reward: -8.282058715820312\n",
      "reward: -4.0540771484375\n",
      "reward: -3.266265869140625\n",
      "reward: -9.916656494140625\n",
      "reward: -3.389312744140625\n",
      "reward: -5.586212158203125\n",
      "reward: -0.46832275390625\n",
      "reward: 3.333221435546875\n",
      "reward: 9.197189331054688\n",
      "reward: 11.3079833984375\n",
      "reward: 6.9361114501953125\n",
      "reward: 10.863128662109375\n",
      "reward: 14.217971801757812\n",
      "reward: 11.107376098632812\n",
      "reward: 0.408905029296875\n",
      "reward: -2.8162994384765625\n",
      "reward: -8.653228759765625\n",
      "reward: -2.4735107421875\n",
      "reward: 2.0509796142578125\n",
      "reward: 1.38330078125\n",
      "reward: 0.932952880859375\n",
      "reward: 2.6560211181640625\n",
      "reward: 2.9773406982421875\n",
      "reward: 2.0081939697265625\n",
      "reward: 4.6266326904296875\n",
      "reward: 4.7886199951171875\n",
      "reward: 3.2299041748046875\n",
      "reward: 2.1784820556640625\n",
      "reward: 0.013885498046875\n",
      "reward: 6.1939849853515625\n",
      "reward: 7.826995849609375\n",
      "reward: 5.2793426513671875\n",
      "reward: 5.01617431640625\n",
      "reward: -2.8015289306640625\n",
      "reward: -5.538970947265625\n",
      "reward: -3.7362213134765625\n",
      "reward: -2.520233154296875\n",
      "reward: 5.0797119140625\n",
      "reward: 6.9865570068359375\n",
      "reward: 4.712432861328125\n",
      "reward: 3.1785430908203125\n",
      "reward: 2.1438751220703125\n",
      "reward: 1.4459991455078125\n",
      "reward: 0.975250244140625\n",
      "reward: -5.02679443359375\n",
      "reward: -2.1823577880859375\n",
      "reward: 6.4602508544921875\n",
      "reward: 13.02679443359375\n",
      "reward: 11.771682739257812\n",
      "reward: 2.8219146728515625\n",
      "reward: 3.9548797607421875\n",
      "reward: 5.156280517578125\n",
      "reward: 3.4779052734375\n",
      "reward: -2.959747314453125\n",
      "reward: -0.3079376220703125\n",
      "reward: 6.61676025390625\n",
      "reward: 6.00250244140625\n",
      "reward: -0.2980499267578125\n",
      "reward: 1.5862579345703125\n",
      "reward: 8.83843994140625\n",
      "reward: 3.2111053466796875\n",
      "reward: 2.84942626953125\n",
      "reward: 1.08740234375\n",
      "reward: 6.5252685546875\n",
      "reward: 3.678497314453125\n",
      "reward: 0.0384979248046875\n",
      "reward: -3.327392578125\n",
      "reward: -5.458831787109375\n",
      "reward: -5.415557861328125\n",
      "reward: -3.6529388427734375\n",
      "reward: 1.734619140625\n",
      "reward: -0.979156494140625\n",
      "reward: 1.36566162109375\n",
      "reward: 3.161895751953125\n",
      "reward: 2.1326446533203125\n",
      "reward: 1.4384307861328125\n",
      "reward: 5.9342498779296875\n",
      "reward: 1.6452789306640625\n",
      "reward: 3.46685791015625\n",
      "reward: 9.909317016601562\n",
      "reward: 4.326507568359375\n",
      "reward: 5.275360107421875\n",
      "reward: 3.611602783203125\n",
      "reward: 0.4141387939453125\n",
      "reward: -2.7178192138671875\n",
      "reward: -3.4071807861328125\n",
      "reward: -4.483123779296875\n",
      "reward: -2.6371917724609375\n",
      "reward: -0.569793701171875\n",
      "reward: -0.384429931640625\n",
      "reward: -6.9654083251953125\n",
      "reward: -8.154067993164062\n",
      "reward: -2.84039306640625\n",
      "reward: 0.26873779296875\n",
      "reward: -0.8563995361328125\n",
      "reward: 4.7723236083984375\n",
      "Result: 325.50355529785156\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: -0.836029052734375\n",
      "reward: -1.0030670166015625\n",
      "reward: -1.240386962890625\n",
      "reward: -0.57562255859375\n",
      "reward: 5.9606170654296875\n",
      "reward: 13.446548461914062\n",
      "reward: 13.045562744140625\n",
      "reward: 9.23828125\n",
      "reward: -2.15655517578125\n",
      "reward: -3.58172607421875\n",
      "reward: -2.403778076171875\n",
      "reward: -1.6214752197265625\n",
      "reward: -2.9111175537109375\n",
      "reward: -2.952117919921875\n",
      "reward: 4.1451416015625\n",
      "reward: 5.9029083251953125\n",
      "reward: 9.969482421875\n",
      "reward: 12.050933837890625\n",
      "reward: 9.31317138671875\n",
      "reward: 4.831512451171875\n",
      "reward: 3.9473724365234375\n",
      "reward: 10.374221801757812\n",
      "reward: 10.6473388671875\n",
      "reward: 7.18170166015625\n",
      "reward: 4.844085693359375\n",
      "reward: 0.9938201904296875\n",
      "reward: -4.1213836669921875\n",
      "reward: -1.708740234375\n",
      "reward: 6.74444580078125\n",
      "reward: 5.8177490234375\n",
      "reward: 2.739166259765625\n",
      "reward: 7.5056304931640625\n",
      "reward: 12.648162841796875\n",
      "reward: 11.645004272460938\n",
      "reward: 7.854644775390625\n",
      "reward: 5.2979888916015625\n",
      "reward: 3.5691986083984375\n",
      "reward: -3.659576416015625\n",
      "reward: -5.6490478515625\n",
      "reward: -9.364547729492188\n",
      "reward: -3.67938232421875\n",
      "reward: 0.434661865234375\n",
      "reward: 5.26104736328125\n",
      "reward: 6.1574554443359375\n",
      "reward: -0.700836181640625\n",
      "reward: -6.680145263671875\n",
      "reward: -1.775146484375\n",
      "reward: -5.6807098388671875\n",
      "reward: -2.1241607666015625\n",
      "reward: 6.016204833984375\n",
      "reward: 6.6227874755859375\n",
      "reward: 3.48480224609375\n",
      "reward: 2.12054443359375\n",
      "reward: -4.309417724609375\n",
      "reward: -9.408279418945312\n",
      "reward: -2.1687774658203125\n",
      "reward: -1.860595703125\n",
      "reward: -2.8229522705078125\n",
      "reward: -1.8460540771484375\n",
      "reward: -6.79931640625\n",
      "reward: -1.9490814208984375\n",
      "reward: -3.9522552490234375\n",
      "reward: -0.0287322998046875\n",
      "reward: 7.1601715087890625\n",
      "reward: 12.62213134765625\n",
      "reward: -96.70770263671875\n",
      "Result: 43.31578063964844\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: -0.209564208984375\n",
      "reward: -7.3618316650390625\n",
      "reward: -5.212982177734375\n",
      "reward: -2.53948974609375\n",
      "reward: -0.66217041015625\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 7.1002655029296875\n",
      "reward: 1.417327880859375\n",
      "reward: -3.186309814453125\n",
      "reward: -2.77996826171875\n",
      "reward: -2.546051025390625\n",
      "reward: -0.0156707763671875\n",
      "reward: 0.413482666015625\n",
      "reward: 0.4959869384765625\n",
      "reward: 0.3344573974609375\n",
      "reward: 7.3253631591796875\n",
      "reward: 1.5693359375\n",
      "reward: -9.770111083984375\n",
      "reward: -0.370635986328125\n",
      "reward: 0.0\n",
      "reward: 0.4185791015625\n",
      "reward: -0.400177001953125\n",
      "reward: -0.018402099609375\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.1912689208984375\n",
      "reward: 0.02203369140625\n",
      "reward: 0.1117401123046875\n",
      "reward: 0.1824951171875\n",
      "reward: -0.4054412841796875\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 1.4552459716796875\n",
      "reward: 1.74566650390625\n",
      "reward: 1.177337646484375\n",
      "reward: -4.46026611328125\n",
      "reward: -0.0144500732421875\n",
      "reward: 6.7797088623046875\n",
      "reward: 8.133193969726562\n",
      "reward: 2.6389923095703125\n",
      "reward: -6.31817626953125\n",
      "reward: -7.656982421875\n",
      "reward: -3.5715179443359375\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 1.50286865234375\n",
      "reward: 1.7882232666015625\n",
      "reward: -3.2381744384765625\n",
      "reward: 2.2187957763671875\n",
      "reward: -0.1063079833984375\n",
      "reward: -1.327606201171875\n",
      "reward: 5.2042999267578125\n",
      "reward: 9.907028198242188\n",
      "reward: 2.2592926025390625\n",
      "reward: 4.420379638671875\n",
      "reward: 6.184783935546875\n",
      "reward: -1.506744384765625\n",
      "reward: -4.9033203125\n",
      "reward: -5.5444183349609375\n",
      "reward: -5.5660400390625\n",
      "reward: -3.6289520263671875\n",
      "reward: 2.5641021728515625\n",
      "reward: 4.273956298828125\n",
      "reward: 2.88275146484375\n",
      "reward: 7.249847412109375\n",
      "reward: 12.9815673828125\n",
      "reward: 6.2366485595703125\n",
      "reward: 5.895172119140625\n",
      "reward: 6.3261260986328125\n",
      "reward: 2.5277862548828125\n",
      "reward: 5.95477294921875\n",
      "reward: 6.9983062744140625\n",
      "reward: -1.3796844482421875\n",
      "reward: -7.32769775390625\n",
      "reward: -6.6198272705078125\n",
      "reward: -1.2718658447265625\n",
      "reward: 0.8189544677734375\n",
      "reward: -1.839141845703125\n",
      "reward: -2.4965057373046875\n",
      "reward: -1.6840362548828125\n",
      "reward: -1.1661529541015625\n",
      "reward: -0.73931884765625\n",
      "reward: -0.4988250732421875\n",
      "reward: -0.3365478515625\n",
      "reward: -0.1246795654296875\n",
      "reward: -100.0\n",
      "Result: -69.097900390625\n"
     ]
    }
   ],
   "source": [
    "# Loop through episodes \n",
    "episodes = 10 \n",
    "for episode in range(episodes): \n",
    "    # Create a new episode or game \n",
    "    game.new_episode()\n",
    "    # Check the game isn't done \n",
    "    while not game.is_episode_finished(): \n",
    "        # Get the game state \n",
    "        state = game.get_state()\n",
    "        # Get the game image \n",
    "        img = state.screen_buffer\n",
    "        # Get the game variables - ammo\n",
    "        info = state.game_variables\n",
    "        # Take an action\n",
    "        reward = game.make_action(random.choice(actions),4)\n",
    "        # Print rewward \n",
    "        print('reward:', reward) \n",
    "        time.sleep(0.02)\n",
    "    print('Result:', game.get_total_reward())\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41b91823-7f6f-472e-973e-bebcf0c95bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "aecbff6a-c71e-4012-8342-bc7d84b99675",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in c:\\users\\indug\\anaconda3\\lib\\site-packages (0.25.1)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\indug\\anaconda3\\lib\\site-packages (from gym) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\indug\\anaconda3\\lib\\site-packages (from gym) (2.2.1)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\indug\\anaconda3\\lib\\site-packages (from gym) (0.0.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c352d76a-07be-4abb-b1a4-7d851bf08239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import environment base class from OpenAI Gym\n",
    "from gym import Env\n",
    "# Import gym spaces \n",
    "from gym.spaces import Discrete, Box\n",
    "# Import opencv \n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "857da86f-8068-4ff2-8ae2-504a99ea768c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# game.get_state().screen_buffer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3b97e80-5c1e-4e20-a078-6413437fecfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Vizdoom OpenAI Gym Environment\n",
    "class VizDoomGym(Env): \n",
    "    # Function that is called when we start the env\n",
    "    def __init__(self, render=False, config= r'C:\\Users\\indug\\doom\\ViZDoom\\scenarios\\deadly_corridor_s1.cfg'): \n",
    "        # Inherit from Env\n",
    "        super().__init__()\n",
    "        # Setup the game \n",
    "        self.game = DoomGame()\n",
    "        self.game.load_config(config)\n",
    "        \n",
    "        # Render frame logic\n",
    "        if render == False: \n",
    "            self.game.set_window_visible(False)\n",
    "        else:\n",
    "            self.game.set_window_visible(True)\n",
    "        \n",
    "        # Start the game \n",
    "        self.game.init()\n",
    "        \n",
    "        # Create the action space and observation space\n",
    "        self.observation_space = Box(low=0, high=255, shape=(100,160,1), dtype=np.uint8) \n",
    "        self.action_space = Discrete(7)\n",
    "        \n",
    "        # Game variables: HEALTH DAMAGE_TAKEN HITCOUNT SELECTED_WEAPON_AMMO\n",
    "        self.damage_taken = 0\n",
    "        self.hitcount = 0\n",
    "        self.ammo = 52 ## CHANGED\n",
    "        \n",
    "        \n",
    "    # This is how we take a step in the environment\n",
    "    def step(self, action):\n",
    "        # Specify action and take step \n",
    "        actions = np.identity(7)\n",
    "        movement_reward = self.game.make_action(actions[action], 4) \n",
    "        \n",
    "        reward = 0 \n",
    "        # Get all the other stuff we need to retun \n",
    "        if self.game.get_state(): \n",
    "            state = self.game.get_state().screen_buffer\n",
    "            state = self.grayscale(state)\n",
    "            \n",
    "            # Reward shaping\n",
    "            game_variables = self.game.get_state().game_variables\n",
    "            health, damage_taken, hitcount, ammo = game_variables\n",
    "            \n",
    "            # Calculate reward deltas\n",
    "            damage_taken_delta = -damage_taken + self.damage_taken\n",
    "            self.damage_taken = damage_taken\n",
    "            hitcount_delta = hitcount - self.hitcount\n",
    "            self.hitcount = hitcount\n",
    "            ammo_delta = ammo - self.ammo\n",
    "            self.ammo = ammo\n",
    "            \n",
    "            reward = movement_reward + damage_taken_delta*10 + hitcount_delta*200  + ammo_delta*5 \n",
    "            info = ammo\n",
    "        else: \n",
    "            state = np.zeros(self.observation_space.shape)\n",
    "            info = 0 \n",
    "        \n",
    "        info = {\"info\":info}\n",
    "        done = self.game.is_episode_finished()\n",
    "        \n",
    "        return state, reward, done, info \n",
    "    \n",
    "    # Define how to render the game or environment \n",
    "    def render(): \n",
    "        pass\n",
    "    \n",
    "    # What happens when we start a new game \n",
    "    def reset(self): \n",
    "        self.game.new_episode()\n",
    "        state = self.game.get_state().screen_buffer\n",
    "        return self.grayscale(state)\n",
    "    \n",
    "    # Grayscale the game frame and resize it \n",
    "    def grayscale(self, observation):\n",
    "        gray = cv2.cvtColor(np.moveaxis(observation, 0, -1), cv2.COLOR_BGR2GRAY)\n",
    "        resize = cv2.resize(gray, (160,100), interpolation=cv2.INTER_CUBIC)\n",
    "        state = np.reshape(resize, (100,160,1))\n",
    "        return state\n",
    "    \n",
    "    # Call to close down the game\n",
    "    def close(self): \n",
    "        self.game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8836339b-680c-41b2-ba66-9d0e51446411",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = VizDoomGym(render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ccefc5b-b75e-4d1b-b89f-dcf94adf652f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[32],\n",
       "         [33],\n",
       "         [25],\n",
       "         ...,\n",
       "         [27],\n",
       "         [23],\n",
       "         [24]],\n",
       " \n",
       "        [[27],\n",
       "         [33],\n",
       "         [23],\n",
       "         ...,\n",
       "         [24],\n",
       "         [24],\n",
       "         [24]],\n",
       " \n",
       "        [[20],\n",
       "         [35],\n",
       "         [23],\n",
       "         ...,\n",
       "         [24],\n",
       "         [24],\n",
       "         [24]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[75],\n",
       "         [63],\n",
       "         [62],\n",
       "         ...,\n",
       "         [44],\n",
       "         [71],\n",
       "         [60]],\n",
       " \n",
       "        [[15],\n",
       "         [48],\n",
       "         [47],\n",
       "         ...,\n",
       "         [49],\n",
       "         [69],\n",
       "         [47]],\n",
       " \n",
       "        [[22],\n",
       "         [14],\n",
       "         [26],\n",
       "         ...,\n",
       "         [57],\n",
       "         [37],\n",
       "         [39]]], dtype=uint8),\n",
       " 0.0,\n",
       " False,\n",
       " {'info': 52.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "882c7629-a1ea-435c-8e12-0cb5e78d30d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32dbabf5-d5b6-4cfd-9f61-85f03a3df18b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[32],\n",
       "        [33],\n",
       "        [25],\n",
       "        ...,\n",
       "        [27],\n",
       "        [23],\n",
       "        [24]],\n",
       "\n",
       "       [[27],\n",
       "        [33],\n",
       "        [23],\n",
       "        ...,\n",
       "        [24],\n",
       "        [24],\n",
       "        [24]],\n",
       "\n",
       "       [[20],\n",
       "        [35],\n",
       "        [23],\n",
       "        ...,\n",
       "        [24],\n",
       "        [24],\n",
       "        [24]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[75],\n",
       "        [63],\n",
       "        [62],\n",
       "        ...,\n",
       "        [44],\n",
       "        [71],\n",
       "        [60]],\n",
       "\n",
       "       [[15],\n",
       "        [48],\n",
       "        [47],\n",
       "        ...,\n",
       "        [49],\n",
       "        [69],\n",
       "        [47]],\n",
       "\n",
       "       [[22],\n",
       "        [14],\n",
       "        [26],\n",
       "        ...,\n",
       "        [57],\n",
       "        [37],\n",
       "        [39]]], dtype=uint8)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c013c96-ded7-44ec-a29c-aabf646c74eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09d6f05f-6ae4-41aa-a990-588c5fc4efe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Environment checker\n",
    "from stable_baselines3.common import env_checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "ccaf3339-b6ee-4e66-9614-ca7e2416743c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env_checker.check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "5d286161-9e3d-439f-9e2b-32afd8d980fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# !pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "4a0674f6-c617-44e6-847a-05621193fb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "3ff5561a-165c-478a-a771-2926cc1eb002",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# plt.imshow(cv2.cvtColor(state, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df7b1fcd-bfcd-4264-bb1b-e5dbdf436811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Requirement already satisfied: torch in c:\\users\\indug\\anaconda3\\lib\\site-packages (2.2.2+cu118)\n",
      "Requirement already satisfied: torchvision in c:\\users\\indug\\anaconda3\\lib\\site-packages (0.17.2+cu118)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\indug\\anaconda3\\lib\\site-packages (2.2.2+cu118)\n",
      "Requirement already satisfied: filelock in c:\\users\\indug\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\indug\\anaconda3\\lib\\site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\indug\\anaconda3\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\indug\\anaconda3\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\indug\\anaconda3\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\indug\\anaconda3\\lib\\site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\indug\\anaconda3\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\indug\\anaconda3\\lib\\site-packages (from torchvision) (10.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\indug\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\indug\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "78cc5083-59ea-4a09-ac7d-25ebda8087d8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stable-baselines3[extra] in c:\\users\\indug\\anaconda3\\lib\\site-packages (2.3.0)\n",
      "Requirement already satisfied: gymnasium<0.30,>=0.28.1 in c:\\users\\indug\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\indug\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (1.26.4)\n",
      "Requirement already satisfied: torch>=1.13 in c:\\users\\indug\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (2.2.2+cu118)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\indug\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (2.2.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\indug\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (2.1.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\indug\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (3.8.0)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\indug\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (4.9.0.80)\n",
      "Requirement already satisfied: pygame in c:\\users\\indug\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (2.5.2)\n",
      "Requirement already satisfied: tensorboard>=2.9.1 in c:\\users\\indug\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (2.16.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\indug\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (5.9.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\indug\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (4.65.0)\n",
      "Requirement already satisfied: rich in c:\\users\\indug\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (13.3.5)\n",
      "Requirement already satisfied: shimmy~=1.3.0 in c:\\users\\indug\\anaconda3\\lib\\site-packages (from shimmy[atari]~=1.3.0; extra == \"extra\"->stable-baselines3[extra]) (1.3.0)\n",
      "Requirement already satisfied: pillow in c:\\users\\indug\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (10.2.0)\n",
      "Requirement already satisfied: autorom~=0.6.1 in c:\\users\\indug\\anaconda3\\lib\\site-packages (from autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]) (0.6.1)\n",
      "Requirement already satisfied: click in c:\\users\\indug\\anaconda3\\lib\\site-packages (from autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]) (8.1.7)\n",
      "Requirement already satisfied: requests in c:\\users\\indug\\anaconda3\\lib\\site-packages (from autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]) (2.31.0)\n",
      "Requirement already satisfied: AutoROM.accept-rom-license in c:\\users\\indug\\anaconda3\\lib\\site-packages (from autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]) (0.6.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\indug\\anaconda3\\lib\\site-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3[extra]) (4.9.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\indug\\anaconda3\\lib\\site-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3[extra]) (0.0.4)\n",
      "Requirement already satisfied: ale-py~=0.8.1 in c:\\users\\indug\\anaconda3\\lib\\site-packages (from shimmy[atari]~=1.3.0; extra == \"extra\"->stable-baselines3[extra]) (0.8.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\indug\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in c:\\users\\indug\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.62.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\indug\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.4.1)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in c:\\users\\indug\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.20.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\indug\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (69.5.1)\n",
      "Requirement already satisfied: six>1.9 in c:\\users\\indug\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\indug\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\indug\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (2.2.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\indug\\anaconda3\\lib\\site-packages (from torch>=1.13->stable-baselines3[extra]) (3.13.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\indug\\anaconda3\\lib\\site-packages (from torch>=1.13->stable-baselines3[extra]) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\indug\\anaconda3\\lib\\site-packages (from torch>=1.13->stable-baselines3[extra]) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\indug\\anaconda3\\lib\\site-packages (from torch>=1.13->stable-baselines3[extra]) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\indug\\anaconda3\\lib\\site-packages (from torch>=1.13->stable-baselines3[extra]) (2023.10.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\indug\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\indug\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\indug\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\indug\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\indug\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (23.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\indug\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\indug\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\indug\\anaconda3\\lib\\site-packages (from pandas->stable-baselines3[extra]) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\indug\\anaconda3\\lib\\site-packages (from pandas->stable-baselines3[extra]) (2023.3)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\indug\\anaconda3\\lib\\site-packages (from rich->stable-baselines3[extra]) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\indug\\anaconda3\\lib\\site-packages (from rich->stable-baselines3[extra]) (2.15.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\indug\\anaconda3\\lib\\site-packages (from tqdm->stable-baselines3[extra]) (0.4.6)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\indug\\anaconda3\\lib\\site-packages (from ale-py~=0.8.1->shimmy[atari]~=1.3.0; extra == \"extra\"->stable-baselines3[extra]) (6.4.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\indug\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->stable-baselines3[extra]) (0.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\indug\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\indug\\anaconda3\\lib\\site-packages (from requests->autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\indug\\anaconda3\\lib\\site-packages (from requests->autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\indug\\anaconda3\\lib\\site-packages (from requests->autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\indug\\anaconda3\\lib\\site-packages (from requests->autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\indug\\anaconda3\\lib\\site-packages (from sympy->torch>=1.13->stable-baselines3[extra]) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install stable-baselines3[extra]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "28b475b7-a949-4d34-a6cb-eaa3c137ee38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import os for file nav\n",
    "import os \n",
    "# Import callback class from sb3\n",
    "from stable_baselines3.common.callbacks import BaseCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c2036c89-18d0-48c0-b184-f0f1ea992711",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "455ef84d-9635-4f02-bfe0-cd59009aa3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = './train/train_corridor'\n",
    "LOG_DIR = './logs/log_corridors1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8445abcd-4652-4e66-bf3f-3103e83567d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = TrainAndLoggingCallback(check_freq=10000, save_path=CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2defadce-c5c9-468f-b9c7-45777b1906d7",
   "metadata": {},
   "source": [
    "#training using circulum learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0833fdd5-47ad-465a-bfa0-baf2729a0741",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import ppo for training\n",
    "from stable_baselines3 import PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "359d0abf-0fbe-4cde-84e8-437de4ac1672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non rendered environment\n",
    "env = VizDoomGym(config = r'C:\\Users\\indug\\doom\\ViZDoom\\scenarios\\deadly_corridor_s2.cfg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d9fba12d-9e6f-4f57-bd5d-dea462135e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "model = PPO('CnnPolicy', env, tensorboard_log=LOG_DIR, verbose=1, learning_rate=0.00001, n_steps=8192, clip_range=.1, gamma=.95, gae_lambda=.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "72f78a4b-f1dc-487b-a346-09a616f8450d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./logs/log_corridors1\\PPO_3\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 201      |\n",
      "|    ep_rew_mean     | 31       |\n",
      "| time/              |          |\n",
      "|    fps             | 19       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 107      |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | 83.1       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 15         |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 262        |\n",
      "|    total_timesteps      | 4096       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01006293 |\n",
      "|    clip_fraction        | 0.187      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.93      |\n",
      "|    explained_variance   | 1.38e-05   |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 5.75e+03   |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | 0.00404    |\n",
      "|    value_loss           | 1.16e+04   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 209         |\n",
      "|    ep_rew_mean          | 97          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 419         |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010254282 |\n",
      "|    clip_fraction        | 0.178       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.93       |\n",
      "|    explained_variance   | 0.0569      |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 4.19e+03    |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | 0.00282     |\n",
      "|    value_loss           | 9.19e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 205         |\n",
      "|    ep_rew_mean          | 133         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 577         |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016031383 |\n",
      "|    clip_fraction        | 0.25        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.91       |\n",
      "|    explained_variance   | 0.0403      |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.97e+03    |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | 0.00746     |\n",
      "|    value_loss           | 7.73e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | 110         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 13          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 734         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038933374 |\n",
      "|    clip_fraction        | 0.278       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.88       |\n",
      "|    explained_variance   | 0.106       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 5.23e+03    |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | 0.005       |\n",
      "|    value_loss           | 1.13e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 203        |\n",
      "|    ep_rew_mean          | 130        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 13         |\n",
      "|    iterations           | 6          |\n",
      "|    time_elapsed         | 889        |\n",
      "|    total_timesteps      | 12288      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09123202 |\n",
      "|    clip_fraction        | 0.393      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.84      |\n",
      "|    explained_variance   | 0.159      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 3.47e+03   |\n",
      "|    n_updates            | 50         |\n",
      "|    policy_gradient_loss | 0.0201     |\n",
      "|    value_loss           | 1.17e+04   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 217         |\n",
      "|    ep_rew_mean          | 133         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 13          |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 1046        |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.061874487 |\n",
      "|    clip_fraction        | 0.616       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.7        |\n",
      "|    explained_variance   | 0.276       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 2.49e+03    |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | 0.0735      |\n",
      "|    value_loss           | 1.09e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 220        |\n",
      "|    ep_rew_mean          | 137        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 13         |\n",
      "|    iterations           | 8          |\n",
      "|    time_elapsed         | 1205       |\n",
      "|    total_timesteps      | 16384      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07344393 |\n",
      "|    clip_fraction        | 0.559      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.7       |\n",
      "|    explained_variance   | 0.299      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 1.91e+03   |\n",
      "|    n_updates            | 70         |\n",
      "|    policy_gradient_loss | 0.0603     |\n",
      "|    value_loss           | 5.73e+03   |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 196        |\n",
      "|    ep_rew_mean          | 118        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 13         |\n",
      "|    iterations           | 9          |\n",
      "|    time_elapsed         | 1333       |\n",
      "|    total_timesteps      | 18432      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.60067785 |\n",
      "|    clip_fraction        | 0.558      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.62      |\n",
      "|    explained_variance   | 0.364      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 2.47e+03   |\n",
      "|    n_updates            | 80         |\n",
      "|    policy_gradient_loss | 0.0426     |\n",
      "|    value_loss           | 6.73e+03   |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 171        |\n",
      "|    ep_rew_mean          | 113        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 13         |\n",
      "|    iterations           | 10         |\n",
      "|    time_elapsed         | 1496       |\n",
      "|    total_timesteps      | 20480      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02202278 |\n",
      "|    clip_fraction        | 0.395      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.45      |\n",
      "|    explained_variance   | 0.329      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 5.63e+03   |\n",
      "|    n_updates            | 90         |\n",
      "|    policy_gradient_loss | 0.0252     |\n",
      "|    value_loss           | 1.31e+04   |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 150       |\n",
      "|    ep_rew_mean          | 85.5      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 13        |\n",
      "|    iterations           | 11        |\n",
      "|    time_elapsed         | 1658      |\n",
      "|    total_timesteps      | 22528     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1530391 |\n",
      "|    clip_fraction        | 0.568     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.42     |\n",
      "|    explained_variance   | 0.54      |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 5.8e+03   |\n",
      "|    n_updates            | 100       |\n",
      "|    policy_gradient_loss | 0.0476    |\n",
      "|    value_loss           | 1.06e+04  |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 135        |\n",
      "|    ep_rew_mean          | 75.4       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 13         |\n",
      "|    iterations           | 12         |\n",
      "|    time_elapsed         | 1818       |\n",
      "|    total_timesteps      | 24576      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10526734 |\n",
      "|    clip_fraction        | 0.509      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.48      |\n",
      "|    explained_variance   | 0.641      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 9.51e+03   |\n",
      "|    n_updates            | 110        |\n",
      "|    policy_gradient_loss | 0.0375     |\n",
      "|    value_loss           | 1.11e+04   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 98.9        |\n",
      "|    ep_rew_mean          | 43.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 13          |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 1982        |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027711133 |\n",
      "|    clip_fraction        | 0.382       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.49       |\n",
      "|    explained_variance   | 0.684       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 2e+03       |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | 0.0201      |\n",
      "|    value_loss           | 9.43e+03    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 102        |\n",
      "|    ep_rew_mean          | 56.7       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 13         |\n",
      "|    iterations           | 14         |\n",
      "|    time_elapsed         | 2142       |\n",
      "|    total_timesteps      | 28672      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.29950178 |\n",
      "|    clip_fraction        | 0.568      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.28      |\n",
      "|    explained_variance   | 0.757      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 6.11e+03   |\n",
      "|    n_updates            | 130        |\n",
      "|    policy_gradient_loss | 0.0556     |\n",
      "|    value_loss           | 7.35e+03   |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 103        |\n",
      "|    ep_rew_mean          | 60.4       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 13         |\n",
      "|    iterations           | 15         |\n",
      "|    time_elapsed         | 2303       |\n",
      "|    total_timesteps      | 30720      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05385886 |\n",
      "|    clip_fraction        | 0.396      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.37      |\n",
      "|    explained_variance   | 0.712      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 5.03e+03   |\n",
      "|    n_updates            | 140        |\n",
      "|    policy_gradient_loss | 0.0289     |\n",
      "|    value_loss           | 1.2e+04    |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 101         |\n",
      "|    ep_rew_mean          | 75.7        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 13          |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 2463        |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029647559 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.31       |\n",
      "|    explained_variance   | 0.767       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 4.76e+03    |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | 0.0132      |\n",
      "|    value_loss           | 9.62e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 95.2        |\n",
      "|    ep_rew_mean          | 99.4        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 13          |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 2620        |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042194888 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.32       |\n",
      "|    explained_variance   | 0.747       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 4.82e+03    |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | 0.0151      |\n",
      "|    value_loss           | 1.02e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 93.9       |\n",
      "|    ep_rew_mean          | 97.9       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 13         |\n",
      "|    iterations           | 18         |\n",
      "|    time_elapsed         | 2773       |\n",
      "|    total_timesteps      | 36864      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09929226 |\n",
      "|    clip_fraction        | 0.383      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.13      |\n",
      "|    explained_variance   | 0.763      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 4.09e+03   |\n",
      "|    n_updates            | 170        |\n",
      "|    policy_gradient_loss | 0.0199     |\n",
      "|    value_loss           | 1.11e+04   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 98.9        |\n",
      "|    ep_rew_mean          | 123         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 13          |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 2926        |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.112828776 |\n",
      "|    clip_fraction        | 0.377       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 0.813       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 5.63e+03    |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | 0.0232      |\n",
      "|    value_loss           | 9.58e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 96.3        |\n",
      "|    ep_rew_mean          | 150         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 13          |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 3078        |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018512053 |\n",
      "|    clip_fraction        | 0.267       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | 0.826       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 3.49e+03    |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | 0.0135      |\n",
      "|    value_loss           | 8.41e+03    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 92         |\n",
      "|    ep_rew_mean          | 187        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 13         |\n",
      "|    iterations           | 21         |\n",
      "|    time_elapsed         | 3230       |\n",
      "|    total_timesteps      | 43008      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03580198 |\n",
      "|    clip_fraction        | 0.314      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.09      |\n",
      "|    explained_variance   | 0.678      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 3.77e+03   |\n",
      "|    n_updates            | 200        |\n",
      "|    policy_gradient_loss | 0.0211     |\n",
      "|    value_loss           | 1.1e+04    |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 78.1        |\n",
      "|    ep_rew_mean          | 317         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 13          |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 3383        |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042518515 |\n",
      "|    clip_fraction        | 0.293       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | 0.658       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 5.11e+03    |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | 0.0287      |\n",
      "|    value_loss           | 1.33e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 55.2       |\n",
      "|    ep_rew_mean          | 433        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 13         |\n",
      "|    iterations           | 23         |\n",
      "|    time_elapsed         | 3537       |\n",
      "|    total_timesteps      | 47104      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07165304 |\n",
      "|    clip_fraction        | 0.393      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.847     |\n",
      "|    explained_variance   | 0.603      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 1.14e+04   |\n",
      "|    n_updates            | 220        |\n",
      "|    policy_gradient_loss | 0.022      |\n",
      "|    value_loss           | 1.7e+04    |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 50.9        |\n",
      "|    ep_rew_mean          | 510         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 13          |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 3689        |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041122116 |\n",
      "|    clip_fraction        | 0.266       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.694      |\n",
      "|    explained_variance   | 0.634       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 6.02e+03    |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | 0.0171      |\n",
      "|    value_loss           | 1.82e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 47.9        |\n",
      "|    ep_rew_mean          | 568         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 13          |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 3843        |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033114053 |\n",
      "|    clip_fraction        | 0.228       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.685      |\n",
      "|    explained_variance   | 0.691       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 7.28e+03    |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | 0.00895     |\n",
      "|    value_loss           | 1.5e+04     |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 40.5       |\n",
      "|    ep_rew_mean          | 658        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 13         |\n",
      "|    iterations           | 26         |\n",
      "|    time_elapsed         | 3996       |\n",
      "|    total_timesteps      | 53248      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10052872 |\n",
      "|    clip_fraction        | 0.26       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.614     |\n",
      "|    explained_variance   | 0.587      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 1.04e+04   |\n",
      "|    n_updates            | 250        |\n",
      "|    policy_gradient_loss | 0.0248     |\n",
      "|    value_loss           | 1.84e+04   |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 40.2       |\n",
      "|    ep_rew_mean          | 795        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 13         |\n",
      "|    iterations           | 27         |\n",
      "|    time_elapsed         | 4149       |\n",
      "|    total_timesteps      | 55296      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15860237 |\n",
      "|    clip_fraction        | 0.247      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.365     |\n",
      "|    explained_variance   | 0.609      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 8.19e+03   |\n",
      "|    n_updates            | 260        |\n",
      "|    policy_gradient_loss | 0.00928    |\n",
      "|    value_loss           | 1.88e+04   |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 42.2       |\n",
      "|    ep_rew_mean          | 1.05e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 13         |\n",
      "|    iterations           | 28         |\n",
      "|    time_elapsed         | 4301       |\n",
      "|    total_timesteps      | 57344      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09482211 |\n",
      "|    clip_fraction        | 0.234      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.267     |\n",
      "|    explained_variance   | 0.453      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 1e+04      |\n",
      "|    n_updates            | 270        |\n",
      "|    policy_gradient_loss | 0.0242     |\n",
      "|    value_loss           | 2.36e+04   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 43          |\n",
      "|    ep_rew_mean          | 1.25e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 13          |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 4453        |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.066904925 |\n",
      "|    clip_fraction        | 0.139       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.215      |\n",
      "|    explained_variance   | 0.294       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.96e+04    |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | 0.0197      |\n",
      "|    value_loss           | 3.78e+04    |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 41.9      |\n",
      "|    ep_rew_mean          | 1.35e+03  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 13        |\n",
      "|    iterations           | 30        |\n",
      "|    time_elapsed         | 4605      |\n",
      "|    total_timesteps      | 61440     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0303072 |\n",
      "|    clip_fraction        | 0.059     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0798   |\n",
      "|    explained_variance   | 0.266     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 2.64e+04  |\n",
      "|    n_updates            | 290       |\n",
      "|    policy_gradient_loss | 0.021     |\n",
      "|    value_loss           | 4.44e+04  |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 41.6       |\n",
      "|    ep_rew_mean          | 1.4e+03    |\n",
      "| time/                   |            |\n",
      "|    fps                  | 13         |\n",
      "|    iterations           | 31         |\n",
      "|    time_elapsed         | 4760       |\n",
      "|    total_timesteps      | 63488      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11062011 |\n",
      "|    clip_fraction        | 0.0334     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0241    |\n",
      "|    explained_variance   | 0.228      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 2.96e+04   |\n",
      "|    n_updates            | 300        |\n",
      "|    policy_gradient_loss | 0.0116     |\n",
      "|    value_loss           | 5.46e+04   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 41.2        |\n",
      "|    ep_rew_mean          | 1.37e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 13          |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 4921        |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004956864 |\n",
      "|    clip_fraction        | 0.00601     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0126     |\n",
      "|    explained_variance   | 0.148       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 2.51e+04    |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | 0.00123     |\n",
      "|    value_loss           | 4.27e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 41.6        |\n",
      "|    ep_rew_mean          | 1.35e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 13          |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 5013        |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008575903 |\n",
      "|    clip_fraction        | 0.00786     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0117     |\n",
      "|    explained_variance   | 0.344       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 2.38e+04    |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | 0.00347     |\n",
      "|    value_loss           | 4.34e+04    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 41.5         |\n",
      "|    ep_rew_mean          | 1.3e+03      |\n",
      "| time/                   |              |\n",
      "|    fps                  | 13           |\n",
      "|    iterations           | 34           |\n",
      "|    time_elapsed         | 5044         |\n",
      "|    total_timesteps      | 69632        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025865189 |\n",
      "|    clip_fraction        | 0.00679      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0112      |\n",
      "|    explained_variance   | 0.403        |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 1.38e+04     |\n",
      "|    n_updates            | 330          |\n",
      "|    policy_gradient_loss | 0.000951     |\n",
      "|    value_loss           | 3.78e+04     |\n",
      "------------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 41.3      |\n",
      "|    ep_rew_mean          | 1.35e+03  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 14        |\n",
      "|    iterations           | 35        |\n",
      "|    time_elapsed         | 5068      |\n",
      "|    total_timesteps      | 71680     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6787685 |\n",
      "|    clip_fraction        | 0.0168    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00707  |\n",
      "|    explained_variance   | 0.529     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 8.25e+03  |\n",
      "|    n_updates            | 340       |\n",
      "|    policy_gradient_loss | 0.0108    |\n",
      "|    value_loss           | 2.57e+04  |\n",
      "---------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 41.6          |\n",
      "|    ep_rew_mean          | 1.41e+03      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 14            |\n",
      "|    iterations           | 36            |\n",
      "|    time_elapsed         | 5092          |\n",
      "|    total_timesteps      | 73728         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1641532e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.65e-06     |\n",
      "|    explained_variance   | 0.294         |\n",
      "|    learning_rate        | 0.0001        |\n",
      "|    loss                 | 1.86e+04      |\n",
      "|    n_updates            | 350           |\n",
      "|    policy_gradient_loss | -1.16e-08     |\n",
      "|    value_loss           | 4.22e+04      |\n",
      "-------------------------------------------\n",
      "--------------------------------------------\n",
      "| rollout/                |                |\n",
      "|    ep_len_mean          | 41.6           |\n",
      "|    ep_rew_mean          | 1.39e+03       |\n",
      "| time/                   |                |\n",
      "|    fps                  | 14             |\n",
      "|    iterations           | 37             |\n",
      "|    time_elapsed         | 5116           |\n",
      "|    total_timesteps      | 75776          |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | -3.4924597e-10 |\n",
      "|    clip_fraction        | 0              |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -5.08e-06      |\n",
      "|    explained_variance   | 0.443          |\n",
      "|    learning_rate        | 0.0001         |\n",
      "|    loss                 | 1.31e+04       |\n",
      "|    n_updates            | 360            |\n",
      "|    policy_gradient_loss | -5.92e-09      |\n",
      "|    value_loss           | 3.26e+04       |\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "| rollout/                |                |\n",
      "|    ep_len_mean          | 41.7           |\n",
      "|    ep_rew_mean          | 1.36e+03       |\n",
      "| time/                   |                |\n",
      "|    fps                  | 15             |\n",
      "|    iterations           | 38             |\n",
      "|    time_elapsed         | 5140           |\n",
      "|    total_timesteps      | 77824          |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | -2.0372681e-10 |\n",
      "|    clip_fraction        | 0              |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -6.35e-06      |\n",
      "|    explained_variance   | 0.441          |\n",
      "|    learning_rate        | 0.0001         |\n",
      "|    loss                 | 1.01e+04       |\n",
      "|    n_updates            | 370            |\n",
      "|    policy_gradient_loss | 6.56e-08       |\n",
      "|    value_loss           | 3.38e+04       |\n",
      "--------------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 41.6      |\n",
      "|    ep_rew_mean          | 1.37e+03  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 15        |\n",
      "|    iterations           | 39        |\n",
      "|    time_elapsed         | 5165      |\n",
      "|    total_timesteps      | 79872     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -2.5e-06  |\n",
      "|    explained_variance   | 0.62      |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 1.41e+04  |\n",
      "|    n_updates            | 380       |\n",
      "|    policy_gradient_loss | -6.49e-08 |\n",
      "|    value_loss           | 2.08e+04  |\n",
      "---------------------------------------\n",
      "--------------------------------------------\n",
      "| rollout/                |                |\n",
      "|    ep_len_mean          | 41.4           |\n",
      "|    ep_rew_mean          | 1.4e+03        |\n",
      "| time/                   |                |\n",
      "|    fps                  | 15             |\n",
      "|    iterations           | 40             |\n",
      "|    time_elapsed         | 5187           |\n",
      "|    total_timesteps      | 81920          |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | -3.2014214e-10 |\n",
      "|    clip_fraction        | 0              |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -2.64e-06      |\n",
      "|    explained_variance   | 0.356          |\n",
      "|    learning_rate        | 0.0001         |\n",
      "|    loss                 | 2.49e+04       |\n",
      "|    n_updates            | 390            |\n",
      "|    policy_gradient_loss | 7.31e-08       |\n",
      "|    value_loss           | 4.57e+04       |\n",
      "--------------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 41.4      |\n",
      "|    ep_rew_mean          | 1.4e+03   |\n",
      "| time/                   |           |\n",
      "|    fps                  | 16        |\n",
      "|    iterations           | 41        |\n",
      "|    time_elapsed         | 5209      |\n",
      "|    total_timesteps      | 83968     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -4.19e-07 |\n",
      "|    explained_variance   | 0.452     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 1.4e+04   |\n",
      "|    n_updates            | 400       |\n",
      "|    policy_gradient_loss | -8.56e-10 |\n",
      "|    value_loss           | 4.01e+04  |\n",
      "---------------------------------------\n",
      "--------------------------------------------\n",
      "| rollout/                |                |\n",
      "|    ep_len_mean          | 41.3           |\n",
      "|    ep_rew_mean          | 1.39e+03       |\n",
      "| time/                   |                |\n",
      "|    fps                  | 16             |\n",
      "|    iterations           | 42             |\n",
      "|    time_elapsed         | 5231           |\n",
      "|    total_timesteps      | 86016          |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | -1.4551915e-10 |\n",
      "|    clip_fraction        | 0              |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -6.49e-07      |\n",
      "|    explained_variance   | 0.491          |\n",
      "|    learning_rate        | 0.0001         |\n",
      "|    loss                 | 1.67e+04       |\n",
      "|    n_updates            | 410            |\n",
      "|    policy_gradient_loss | -3.32e-09      |\n",
      "|    value_loss           | 3.16e+04       |\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "| rollout/                |                |\n",
      "|    ep_len_mean          | 41.4           |\n",
      "|    ep_rew_mean          | 1.39e+03       |\n",
      "| time/                   |                |\n",
      "|    fps                  | 16             |\n",
      "|    iterations           | 43             |\n",
      "|    time_elapsed         | 5253           |\n",
      "|    total_timesteps      | 88064          |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | -1.8044375e-09 |\n",
      "|    clip_fraction        | 0              |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -8.35e-07      |\n",
      "|    explained_variance   | 0.48           |\n",
      "|    learning_rate        | 0.0001         |\n",
      "|    loss                 | 2.06e+04       |\n",
      "|    n_updates            | 420            |\n",
      "|    policy_gradient_loss | 4.46e-09       |\n",
      "|    value_loss           | 3.88e+04       |\n",
      "--------------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 41.7      |\n",
      "|    ep_rew_mean          | 1.38e+03  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 17        |\n",
      "|    iterations           | 44        |\n",
      "|    time_elapsed         | 5277      |\n",
      "|    total_timesteps      | 90112     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.74e-06 |\n",
      "|    explained_variance   | 0.437     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 1.56e+04  |\n",
      "|    n_updates            | 430       |\n",
      "|    policy_gradient_loss | 1.52e-08  |\n",
      "|    value_loss           | 4.38e+04  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 41.5      |\n",
      "|    ep_rew_mean          | 1.36e+03  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 17        |\n",
      "|    iterations           | 45        |\n",
      "|    time_elapsed         | 5299      |\n",
      "|    total_timesteps      | 92160     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.03e-06 |\n",
      "|    explained_variance   | 0.558     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 1.11e+04  |\n",
      "|    n_updates            | 440       |\n",
      "|    policy_gradient_loss | 1.38e-08  |\n",
      "|    value_loss           | 3.15e+04  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 41.5      |\n",
      "|    ep_rew_mean          | 1.33e+03  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 17        |\n",
      "|    iterations           | 46        |\n",
      "|    time_elapsed         | 5321      |\n",
      "|    total_timesteps      | 94208     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -4.62e-07 |\n",
      "|    explained_variance   | 0.532     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 1.95e+04  |\n",
      "|    n_updates            | 450       |\n",
      "|    policy_gradient_loss | 7.96e-09  |\n",
      "|    value_loss           | 2.95e+04  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 41.7      |\n",
      "|    ep_rew_mean          | 1.34e+03  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 18        |\n",
      "|    iterations           | 47        |\n",
      "|    time_elapsed         | 5343      |\n",
      "|    total_timesteps      | 96256     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -6.56e-09 |\n",
      "|    explained_variance   | 0.578     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 1.34e+04  |\n",
      "|    n_updates            | 460       |\n",
      "|    policy_gradient_loss | 1.01e-09  |\n",
      "|    value_loss           | 2.84e+04  |\n",
      "---------------------------------------\n",
      "--------------------------------------------\n",
      "| rollout/                |                |\n",
      "|    ep_len_mean          | 41.6           |\n",
      "|    ep_rew_mean          | 1.36e+03       |\n",
      "| time/                   |                |\n",
      "|    fps                  | 18             |\n",
      "|    iterations           | 48             |\n",
      "|    time_elapsed         | 5366           |\n",
      "|    total_timesteps      | 98304          |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | -5.2386895e-10 |\n",
      "|    clip_fraction        | 0              |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -4e-07         |\n",
      "|    explained_variance   | 0.474          |\n",
      "|    learning_rate        | 0.0001         |\n",
      "|    loss                 | 2.27e+04       |\n",
      "|    n_updates            | 470            |\n",
      "|    policy_gradient_loss | 6.2e-10        |\n",
      "|    value_loss           | 3.79e+04       |\n",
      "--------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 41.8         |\n",
      "|    ep_rew_mean          | 1.34e+03     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 18           |\n",
      "|    iterations           | 49           |\n",
      "|    time_elapsed         | 5388         |\n",
      "|    total_timesteps      | 100352       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.947651e-10 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.49e-06    |\n",
      "|    explained_variance   | 0.476        |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 2.13e+04     |\n",
      "|    n_updates            | 480          |\n",
      "|    policy_gradient_loss | 1.29e-08     |\n",
      "|    value_loss           | 3.59e+04     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x23796fc8050>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=400000, callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "04479054-cbb6-4f71-a824-c72312d97869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import eval policy to test agent\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5819e763-2387-4264-9a62-7a0f7b458760",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reload model from disc\n",
    "model = PPO.load('./train/train_corridor/best_model_60000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0429e528-6697-4e1c-8d8c-48ab82123c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create rendered environment\n",
    "env = VizDoomGym(render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b5d22dfa-7064-4c30-8222-269c0cafd789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate mean reward for 10 games\n",
    "mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "90bfc060-aeb7-4c0b-9e7a-3ff0d13dc80e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.4269256591796875"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "mean_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a68f3da-ad09-4a74-b74b-aee2b022bb43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0b2caf71-bca9-4f90-b4d5-3a91b8ddb625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Reward for episode 0 is 730.9943542480469\n",
      "Total Reward for episode 1 is 1585.7324676513672\n",
      "Total Reward for episode 2 is 1291.2153778076172\n",
      "Total Reward for episode 3 is 1233.3893127441406\n",
      "Total Reward for episode 4 is 786.0104370117188\n",
      "Total Reward for episode 5 is 637.8748626708984\n",
      "Total Reward for episode 6 is 1482.8897552490234\n",
      "Total Reward for episode 7 is 735.5814971923828\n",
      "Total Reward for episode 8 is 1661.4496002197266\n",
      "Total Reward for episode 9 is 1370.3328704833984\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for episode in range(10): \n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done: \n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        time.sleep(0.80)\n",
    "        total_reward += reward\n",
    "    print('Total Reward for episode {} is {}'.format(episode, total_reward))\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "232e1f1e-eb8e-4b7d-a931-abff8d3e035f",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "2969383b-2311-4a6c-8637-6c74b280eae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = VizDoomGym(render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "dea13123-3087-4d14-a464-41a343a52517",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[ 6],\n",
       "         [10],\n",
       "         [ 9],\n",
       "         ...,\n",
       "         [ 6],\n",
       "         [11],\n",
       "         [ 4]],\n",
       " \n",
       "        [[ 9],\n",
       "         [ 9],\n",
       "         [ 6],\n",
       "         ...,\n",
       "         [ 6],\n",
       "         [ 6],\n",
       "         [ 4]],\n",
       " \n",
       "        [[10],\n",
       "         [ 7],\n",
       "         [ 7],\n",
       "         ...,\n",
       "         [ 6],\n",
       "         [ 6],\n",
       "         [ 9]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[75],\n",
       "         [63],\n",
       "         [62],\n",
       "         ...,\n",
       "         [44],\n",
       "         [71],\n",
       "         [60]],\n",
       " \n",
       "        [[15],\n",
       "         [48],\n",
       "         [47],\n",
       "         ...,\n",
       "         [49],\n",
       "         [69],\n",
       "         [47]],\n",
       " \n",
       "        [[22],\n",
       "         [14],\n",
       "         [26],\n",
       "         ...,\n",
       "         [57],\n",
       "         [37],\n",
       "         [39]]], dtype=uint8),\n",
       " 0.0,\n",
       " False,\n",
       " {'info': 26.0})"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "fbcc59a2-0cf1-49b7-a9fa-4c3be456a953",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9afea5-f606-42a5-8a2f-bbafb74a5a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load('./train/train_corridor/best_model_60000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f1b5d411-723f-4b79-843c-a2d946ae6cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "Logging to ./logs/log_corridors1\\PPO_6\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 82.1     |\n",
      "|    ep_rew_mean     | 42.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 277      |\n",
      "|    total_timesteps | 8192     |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 92.3         |\n",
      "|    ep_rew_mean          | 76.5         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 43           |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 374          |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024113785 |\n",
      "|    clip_fraction        | 0.0972       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.94        |\n",
      "|    explained_variance   | -6.79e-05    |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 2.68e+03     |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00153     |\n",
      "|    value_loss           | 1.92e+04     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 75.8        |\n",
      "|    ep_rew_mean          | 78.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 53          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 456         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002532267 |\n",
      "|    clip_fraction        | 0.15        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.94       |\n",
      "|    explained_variance   | 0.0234      |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 3.31e+03    |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.000732   |\n",
      "|    value_loss           | 1.61e+04    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 68.1         |\n",
      "|    ep_rew_mean          | 87.2         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 60           |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 537          |\n",
      "|    total_timesteps      | 32768        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026447903 |\n",
      "|    clip_fraction        | 0.162        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.93        |\n",
      "|    explained_variance   | 0.0661       |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 4.57e+03     |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | 0.000391     |\n",
      "|    value_loss           | 1.85e+04     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 68.4         |\n",
      "|    ep_rew_mean          | 118          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 66           |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 619          |\n",
      "|    total_timesteps      | 40960        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030150802 |\n",
      "|    clip_fraction        | 0.204        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.92        |\n",
      "|    explained_variance   | 0.15         |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 1.04e+04     |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | 0.000324     |\n",
      "|    value_loss           | 1.86e+04     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 76.8         |\n",
      "|    ep_rew_mean          | 95.9         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 706          |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032378389 |\n",
      "|    clip_fraction        | 0.219        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.91        |\n",
      "|    explained_variance   | 0.192        |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 9.81e+03     |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | 0.0013       |\n",
      "|    value_loss           | 1.99e+04     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 70.5         |\n",
      "|    ep_rew_mean          | 115          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 68           |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 836          |\n",
      "|    total_timesteps      | 57344        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038318357 |\n",
      "|    clip_fraction        | 0.265        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.91        |\n",
      "|    explained_variance   | 0.228        |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 1.09e+04     |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | 0.00182      |\n",
      "|    value_loss           | 1.78e+04     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 66           |\n",
      "|    ep_rew_mean          | 121          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 942          |\n",
      "|    total_timesteps      | 65536        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032626907 |\n",
      "|    clip_fraction        | 0.245        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.89        |\n",
      "|    explained_variance   | 0.261        |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 4.7e+03      |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | 0.000711     |\n",
      "|    value_loss           | 1.8e+04      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 73.3         |\n",
      "|    ep_rew_mean          | 122          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 70           |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 1046         |\n",
      "|    total_timesteps      | 73728        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063096043 |\n",
      "|    clip_fraction        | 0.294        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.88        |\n",
      "|    explained_variance   | 0.291        |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 5.44e+03     |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | 0.00167      |\n",
      "|    value_loss           | 1.87e+04     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 68.5        |\n",
      "|    ep_rew_mean          | 154         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 72          |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 1131        |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004152991 |\n",
      "|    clip_fraction        | 0.284       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.88       |\n",
      "|    explained_variance   | 0.266       |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 8.19e+03    |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | 0.00236     |\n",
      "|    value_loss           | 1.79e+04    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 67.5         |\n",
      "|    ep_rew_mean          | 183          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 1213         |\n",
      "|    total_timesteps      | 90112        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046378295 |\n",
      "|    clip_fraction        | 0.313        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.87        |\n",
      "|    explained_variance   | 0.314        |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 9.17e+03     |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | 0.00226      |\n",
      "|    value_loss           | 1.81e+04     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 64          |\n",
      "|    ep_rew_mean          | 198         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 75          |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 1297        |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006156511 |\n",
      "|    clip_fraction        | 0.288       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.85       |\n",
      "|    explained_variance   | 0.318       |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 7.33e+03    |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | 0.00111     |\n",
      "|    value_loss           | 1.68e+04    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 71.1         |\n",
      "|    ep_rew_mean          | 271          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 76           |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 1394         |\n",
      "|    total_timesteps      | 106496       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062071113 |\n",
      "|    clip_fraction        | 0.31         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.82        |\n",
      "|    explained_variance   | 0.33         |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 7.34e+03     |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | 0.00309      |\n",
      "|    value_loss           | 1.67e+04     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 68.4        |\n",
      "|    ep_rew_mean          | 313         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 77          |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 1476        |\n",
      "|    total_timesteps      | 114688      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011394691 |\n",
      "|    clip_fraction        | 0.327       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.78       |\n",
      "|    explained_variance   | 0.284       |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 4.9e+03     |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | 0.00319     |\n",
      "|    value_loss           | 1.51e+04    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 69.9         |\n",
      "|    ep_rew_mean          | 322          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 78           |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 1559         |\n",
      "|    total_timesteps      | 122880       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049030455 |\n",
      "|    clip_fraction        | 0.312        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.72        |\n",
      "|    explained_variance   | 0.315        |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 6.75e+03     |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | 0.00249      |\n",
      "|    value_loss           | 1.74e+04     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 64.9         |\n",
      "|    ep_rew_mean          | 352          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 79           |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 1644         |\n",
      "|    total_timesteps      | 131072       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059944503 |\n",
      "|    clip_fraction        | 0.322        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.73        |\n",
      "|    explained_variance   | 0.324        |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 1.43e+04     |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | 0.00317      |\n",
      "|    value_loss           | 1.53e+04     |\n",
      "------------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 67.3      |\n",
      "|    ep_rew_mean          | 346       |\n",
      "| time/                   |           |\n",
      "|    fps                  | 80        |\n",
      "|    iterations           | 17        |\n",
      "|    time_elapsed         | 1725      |\n",
      "|    total_timesteps      | 139264    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0051776 |\n",
      "|    clip_fraction        | 0.297     |\n",
      "|    clip_range           | 0.1       |\n",
      "|    entropy_loss         | -1.71     |\n",
      "|    explained_variance   | 0.328     |\n",
      "|    learning_rate        | 1e-05     |\n",
      "|    loss                 | 4.86e+03  |\n",
      "|    n_updates            | 160       |\n",
      "|    policy_gradient_loss | 0.00238   |\n",
      "|    value_loss           | 1.84e+04  |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 63.1        |\n",
      "|    ep_rew_mean          | 340         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 78          |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 1875        |\n",
      "|    total_timesteps      | 147456      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006388396 |\n",
      "|    clip_fraction        | 0.31        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.71       |\n",
      "|    explained_variance   | 0.356       |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 6.76e+03    |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | 0.00141     |\n",
      "|    value_loss           | 1.74e+04    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 70.9         |\n",
      "|    ep_rew_mean          | 346          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 76           |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 2032         |\n",
      "|    total_timesteps      | 155648       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058762236 |\n",
      "|    clip_fraction        | 0.298        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.69        |\n",
      "|    explained_variance   | 0.323        |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 8.67e+03     |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | 0.00303      |\n",
      "|    value_loss           | 1.75e+04     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 66.9         |\n",
      "|    ep_rew_mean          | 378          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 20           |\n",
      "|    time_elapsed         | 2198         |\n",
      "|    total_timesteps      | 163840       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050219907 |\n",
      "|    clip_fraction        | 0.299        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.65        |\n",
      "|    explained_variance   | 0.32         |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 3.89e+03     |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | 0.00303      |\n",
      "|    value_loss           | 1.59e+04     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 65.6         |\n",
      "|    ep_rew_mean          | 384          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 72           |\n",
      "|    iterations           | 21           |\n",
      "|    time_elapsed         | 2361         |\n",
      "|    total_timesteps      | 172032       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058361595 |\n",
      "|    clip_fraction        | 0.293        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.64        |\n",
      "|    explained_variance   | 0.336        |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 7.04e+03     |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | 0.00392      |\n",
      "|    value_loss           | 1.49e+04     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 64          |\n",
      "|    ep_rew_mean          | 387         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 71          |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 2520        |\n",
      "|    total_timesteps      | 180224      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006301831 |\n",
      "|    clip_fraction        | 0.269       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.59       |\n",
      "|    explained_variance   | 0.371       |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 6.72e+03    |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | 0.00284     |\n",
      "|    value_loss           | 1.66e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 69.5        |\n",
      "|    ep_rew_mean          | 400         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 70          |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 2683        |\n",
      "|    total_timesteps      | 188416      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005678513 |\n",
      "|    clip_fraction        | 0.288       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.56       |\n",
      "|    explained_variance   | 0.424       |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 6.85e+03    |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | 0.00242     |\n",
      "|    value_loss           | 1.63e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 72.1        |\n",
      "|    ep_rew_mean          | 384         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 66          |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 2957        |\n",
      "|    total_timesteps      | 196608      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005588523 |\n",
      "|    clip_fraction        | 0.308       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.58       |\n",
      "|    explained_variance   | 0.42        |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 8.41e+03    |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | 0.00435     |\n",
      "|    value_loss           | 1.62e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 66.1        |\n",
      "|    ep_rew_mean          | 416         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 62          |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 3261        |\n",
      "|    total_timesteps      | 204800      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006041782 |\n",
      "|    clip_fraction        | 0.316       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.55       |\n",
      "|    explained_variance   | 0.428       |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 5.73e+03    |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | 0.00438     |\n",
      "|    value_loss           | 1.42e+04    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 70           |\n",
      "|    ep_rew_mean          | 410          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 58           |\n",
      "|    iterations           | 26           |\n",
      "|    time_elapsed         | 3610         |\n",
      "|    total_timesteps      | 212992       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047998084 |\n",
      "|    clip_fraction        | 0.264        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.53        |\n",
      "|    explained_variance   | 0.393        |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 1.23e+04     |\n",
      "|    n_updates            | 250          |\n",
      "|    policy_gradient_loss | 0.00354      |\n",
      "|    value_loss           | 1.51e+04     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 68.9         |\n",
      "|    ep_rew_mean          | 392          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 56           |\n",
      "|    iterations           | 27           |\n",
      "|    time_elapsed         | 3931         |\n",
      "|    total_timesteps      | 221184       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041467026 |\n",
      "|    clip_fraction        | 0.287        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.52        |\n",
      "|    explained_variance   | 0.386        |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 9.85e+03     |\n",
      "|    n_updates            | 260          |\n",
      "|    policy_gradient_loss | 0.0039       |\n",
      "|    value_loss           | 1.58e+04     |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 65.4       |\n",
      "|    ep_rew_mean          | 402        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 53         |\n",
      "|    iterations           | 28         |\n",
      "|    time_elapsed         | 4267       |\n",
      "|    total_timesteps      | 229376     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00524678 |\n",
      "|    clip_fraction        | 0.272      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | -1.47      |\n",
      "|    explained_variance   | 0.406      |\n",
      "|    learning_rate        | 1e-05      |\n",
      "|    loss                 | 4.35e+03   |\n",
      "|    n_updates            | 270        |\n",
      "|    policy_gradient_loss | 0.00378    |\n",
      "|    value_loss           | 1.6e+04    |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 75.6        |\n",
      "|    ep_rew_mean          | 428         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 52          |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 4505        |\n",
      "|    total_timesteps      | 237568      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005343991 |\n",
      "|    clip_fraction        | 0.265       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.47       |\n",
      "|    explained_variance   | 0.415       |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 9.67e+03    |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | 0.00431     |\n",
      "|    value_loss           | 1.57e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 76.2        |\n",
      "|    ep_rew_mean          | 400         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 52          |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 4677        |\n",
      "|    total_timesteps      | 245760      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005110084 |\n",
      "|    clip_fraction        | 0.294       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.45       |\n",
      "|    explained_variance   | 0.406       |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 5.35e+03    |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | 0.00622     |\n",
      "|    value_loss           | 1.32e+04    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 91.5         |\n",
      "|    ep_rew_mean          | 402          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 52           |\n",
      "|    iterations           | 31           |\n",
      "|    time_elapsed         | 4876         |\n",
      "|    total_timesteps      | 253952       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049221404 |\n",
      "|    clip_fraction        | 0.299        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | 0.437        |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 9.36e+03     |\n",
      "|    n_updates            | 300          |\n",
      "|    policy_gradient_loss | 0.00527      |\n",
      "|    value_loss           | 1.37e+04     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 94          |\n",
      "|    ep_rew_mean          | 411         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 50          |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 5175        |\n",
      "|    total_timesteps      | 262144      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006736984 |\n",
      "|    clip_fraction        | 0.25        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.36       |\n",
      "|    explained_variance   | 0.411       |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 2.39e+03    |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | 0.00572     |\n",
      "|    value_loss           | 1.04e+04    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 81.7         |\n",
      "|    ep_rew_mean          | 415          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 50           |\n",
      "|    iterations           | 33           |\n",
      "|    time_elapsed         | 5357         |\n",
      "|    total_timesteps      | 270336       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062948754 |\n",
      "|    clip_fraction        | 0.268        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.32        |\n",
      "|    explained_variance   | 0.379        |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 4.3e+03      |\n",
      "|    n_updates            | 320          |\n",
      "|    policy_gradient_loss | 0.00721      |\n",
      "|    value_loss           | 1.07e+04     |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 89.2       |\n",
      "|    ep_rew_mean          | 432        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 50         |\n",
      "|    iterations           | 34         |\n",
      "|    time_elapsed         | 5517       |\n",
      "|    total_timesteps      | 278528     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00509533 |\n",
      "|    clip_fraction        | 0.245      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | -1.26      |\n",
      "|    explained_variance   | 0.399      |\n",
      "|    learning_rate        | 1e-05      |\n",
      "|    loss                 | 4.06e+03   |\n",
      "|    n_updates            | 330        |\n",
      "|    policy_gradient_loss | 0.0038     |\n",
      "|    value_loss           | 1.22e+04   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 75.5        |\n",
      "|    ep_rew_mean          | 424         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 50          |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 5679        |\n",
      "|    total_timesteps      | 286720      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006454993 |\n",
      "|    clip_fraction        | 0.25        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.25       |\n",
      "|    explained_variance   | 0.428       |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 6.19e+03    |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | 0.00462     |\n",
      "|    value_loss           | 1.12e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 65.1        |\n",
      "|    ep_rew_mean          | 453         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 50          |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 5847        |\n",
      "|    total_timesteps      | 294912      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011560174 |\n",
      "|    clip_fraction        | 0.255       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.26       |\n",
      "|    explained_variance   | 0.463       |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 9.15e+03    |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | 0.00492     |\n",
      "|    value_loss           | 1.39e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 83.4        |\n",
      "|    ep_rew_mean          | 483         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 50          |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 6013        |\n",
      "|    total_timesteps      | 303104      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007832646 |\n",
      "|    clip_fraction        | 0.267       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.22       |\n",
      "|    explained_variance   | 0.491       |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 7.31e+03    |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | 0.00739     |\n",
      "|    value_loss           | 1.42e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 79.5        |\n",
      "|    ep_rew_mean          | 462         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 50          |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 6187        |\n",
      "|    total_timesteps      | 311296      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005225316 |\n",
      "|    clip_fraction        | 0.26        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.21       |\n",
      "|    explained_variance   | 0.432       |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 3.15e+03    |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | 0.00485     |\n",
      "|    value_loss           | 1.27e+04    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 73           |\n",
      "|    ep_rew_mean          | 465          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 50           |\n",
      "|    iterations           | 39           |\n",
      "|    time_elapsed         | 6358         |\n",
      "|    total_timesteps      | 319488       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063790344 |\n",
      "|    clip_fraction        | 0.292        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.21        |\n",
      "|    explained_variance   | 0.455        |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 4.55e+03     |\n",
      "|    n_updates            | 380          |\n",
      "|    policy_gradient_loss | 0.00803      |\n",
      "|    value_loss           | 1.33e+04     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 79.3        |\n",
      "|    ep_rew_mean          | 497         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 50          |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 6520        |\n",
      "|    total_timesteps      | 327680      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006623412 |\n",
      "|    clip_fraction        | 0.282       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.2        |\n",
      "|    explained_variance   | 0.504       |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 4.15e+03    |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | 0.00786     |\n",
      "|    value_loss           | 1.33e+04    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 79.5         |\n",
      "|    ep_rew_mean          | 475          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 50           |\n",
      "|    iterations           | 41           |\n",
      "|    time_elapsed         | 6684         |\n",
      "|    total_timesteps      | 335872       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056085885 |\n",
      "|    clip_fraction        | 0.279        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.2         |\n",
      "|    explained_variance   | 0.483        |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 4.81e+03     |\n",
      "|    n_updates            | 400          |\n",
      "|    policy_gradient_loss | 0.00704      |\n",
      "|    value_loss           | 1.41e+04     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 72.9        |\n",
      "|    ep_rew_mean          | 467         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 50          |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 6848        |\n",
      "|    total_timesteps      | 344064      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005540481 |\n",
      "|    clip_fraction        | 0.243       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.22       |\n",
      "|    explained_variance   | 0.488       |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 5.4e+03     |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | 0.00638     |\n",
      "|    value_loss           | 1.37e+04    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 97.4         |\n",
      "|    ep_rew_mean          | 491          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 50           |\n",
      "|    iterations           | 43           |\n",
      "|    time_elapsed         | 7009         |\n",
      "|    total_timesteps      | 352256       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057980716 |\n",
      "|    clip_fraction        | 0.289        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.18        |\n",
      "|    explained_variance   | 0.495        |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 6.88e+03     |\n",
      "|    n_updates            | 420          |\n",
      "|    policy_gradient_loss | 0.00633      |\n",
      "|    value_loss           | 1.46e+04     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 127          |\n",
      "|    ep_rew_mean          | 468          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 50           |\n",
      "|    iterations           | 44           |\n",
      "|    time_elapsed         | 7179         |\n",
      "|    total_timesteps      | 360448       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070546437 |\n",
      "|    clip_fraction        | 0.32         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.22        |\n",
      "|    explained_variance   | 0.443        |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 4.95e+03     |\n",
      "|    n_updates            | 430          |\n",
      "|    policy_gradient_loss | 0.0102       |\n",
      "|    value_loss           | 1.1e+04      |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 129         |\n",
      "|    ep_rew_mean          | 448         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 50          |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 7338        |\n",
      "|    total_timesteps      | 368640      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005178716 |\n",
      "|    clip_fraction        | 0.282       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.25       |\n",
      "|    explained_variance   | 0.477       |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 3.67e+03    |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | 0.00631     |\n",
      "|    value_loss           | 7.6e+03     |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 104          |\n",
      "|    ep_rew_mean          | 482          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 50           |\n",
      "|    iterations           | 46           |\n",
      "|    time_elapsed         | 7499         |\n",
      "|    total_timesteps      | 376832       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060586855 |\n",
      "|    clip_fraction        | 0.276        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.19        |\n",
      "|    explained_variance   | 0.44         |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 3.81e+03     |\n",
      "|    n_updates            | 450          |\n",
      "|    policy_gradient_loss | 0.00591      |\n",
      "|    value_loss           | 8.8e+03      |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 96.7        |\n",
      "|    ep_rew_mean          | 455         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 50          |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 7663        |\n",
      "|    total_timesteps      | 385024      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006917496 |\n",
      "|    clip_fraction        | 0.261       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.12       |\n",
      "|    explained_variance   | 0.47        |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 3.63e+03    |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | 0.00528     |\n",
      "|    value_loss           | 9.57e+03    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 102          |\n",
      "|    ep_rew_mean          | 472          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 50           |\n",
      "|    iterations           | 48           |\n",
      "|    time_elapsed         | 7823         |\n",
      "|    total_timesteps      | 393216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046132025 |\n",
      "|    clip_fraction        | 0.266        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.13        |\n",
      "|    explained_variance   | 0.474        |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 3.95e+03     |\n",
      "|    n_updates            | 470          |\n",
      "|    policy_gradient_loss | 0.00611      |\n",
      "|    value_loss           | 1.01e+04     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 99.9        |\n",
      "|    ep_rew_mean          | 503         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 50          |\n",
      "|    iterations           | 49          |\n",
      "|    time_elapsed         | 7984        |\n",
      "|    total_timesteps      | 401408      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005225019 |\n",
      "|    clip_fraction        | 0.26        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.11       |\n",
      "|    explained_variance   | 0.476       |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 4.3e+03     |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | 0.0031      |\n",
      "|    value_loss           | 1.02e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 124         |\n",
      "|    ep_rew_mean          | 496         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 50          |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 8186        |\n",
      "|    total_timesteps      | 409600      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008297533 |\n",
      "|    clip_fraction        | 0.272       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.15       |\n",
      "|    explained_variance   | 0.466       |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 4.44e+03    |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | 0.00553     |\n",
      "|    value_loss           | 1.02e+04    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 138          |\n",
      "|    ep_rew_mean          | 501          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 49           |\n",
      "|    iterations           | 51           |\n",
      "|    time_elapsed         | 8390         |\n",
      "|    total_timesteps      | 417792       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0082482435 |\n",
      "|    clip_fraction        | 0.281        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.14        |\n",
      "|    explained_variance   | 0.436        |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 3.53e+03     |\n",
      "|    n_updates            | 500          |\n",
      "|    policy_gradient_loss | 0.00658      |\n",
      "|    value_loss           | 7.62e+03     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 136         |\n",
      "|    ep_rew_mean          | 497         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 47          |\n",
      "|    iterations           | 52          |\n",
      "|    time_elapsed         | 8976        |\n",
      "|    total_timesteps      | 425984      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006952041 |\n",
      "|    clip_fraction        | 0.256       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.15       |\n",
      "|    explained_variance   | 0.419       |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 3.44e+03    |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | 0.00563     |\n",
      "|    value_loss           | 8.64e+03    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 141          |\n",
      "|    ep_rew_mean          | 539          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 45           |\n",
      "|    iterations           | 53           |\n",
      "|    time_elapsed         | 9550         |\n",
      "|    total_timesteps      | 434176       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051573734 |\n",
      "|    clip_fraction        | 0.234        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.13        |\n",
      "|    explained_variance   | 0.377        |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 5.86e+03     |\n",
      "|    n_updates            | 520          |\n",
      "|    policy_gradient_loss | 0.00511      |\n",
      "|    value_loss           | 8.25e+03     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 132          |\n",
      "|    ep_rew_mean          | 553          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 43           |\n",
      "|    iterations           | 54           |\n",
      "|    time_elapsed         | 10117        |\n",
      "|    total_timesteps      | 442368       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075741657 |\n",
      "|    clip_fraction        | 0.26         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.11        |\n",
      "|    explained_variance   | 0.423        |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 3.83e+03     |\n",
      "|    n_updates            | 530          |\n",
      "|    policy_gradient_loss | 0.00367      |\n",
      "|    value_loss           | 7.91e+03     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 131          |\n",
      "|    ep_rew_mean          | 576          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 42           |\n",
      "|    iterations           | 55           |\n",
      "|    time_elapsed         | 10690        |\n",
      "|    total_timesteps      | 450560       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049481886 |\n",
      "|    clip_fraction        | 0.232        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.12        |\n",
      "|    explained_variance   | 0.431        |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 4.32e+03     |\n",
      "|    n_updates            | 540          |\n",
      "|    policy_gradient_loss | 0.00297      |\n",
      "|    value_loss           | 7.97e+03     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 134         |\n",
      "|    ep_rew_mean          | 559         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 41          |\n",
      "|    iterations           | 56          |\n",
      "|    time_elapsed         | 11114       |\n",
      "|    total_timesteps      | 458752      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006990604 |\n",
      "|    clip_fraction        | 0.267       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.1        |\n",
      "|    explained_variance   | 0.433       |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 5.49e+03    |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | 0.00503     |\n",
      "|    value_loss           | 9.65e+03    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 112          |\n",
      "|    ep_rew_mean          | 549          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 11           |\n",
      "|    iterations           | 57           |\n",
      "|    time_elapsed         | 40954        |\n",
      "|    total_timesteps      | 466944       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059521142 |\n",
      "|    clip_fraction        | 0.263        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.13        |\n",
      "|    explained_variance   | 0.408        |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 7.52e+03     |\n",
      "|    n_updates            | 560          |\n",
      "|    policy_gradient_loss | 0.00913      |\n",
      "|    value_loss           | 9.31e+03     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 111          |\n",
      "|    ep_rew_mean          | 592          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 11           |\n",
      "|    iterations           | 58           |\n",
      "|    time_elapsed         | 41051        |\n",
      "|    total_timesteps      | 475136       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054276576 |\n",
      "|    clip_fraction        | 0.233        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.1         |\n",
      "|    explained_variance   | 0.458        |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 4.73e+03     |\n",
      "|    n_updates            | 570          |\n",
      "|    policy_gradient_loss | 0.00187      |\n",
      "|    value_loss           | 9.79e+03     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 114          |\n",
      "|    ep_rew_mean          | 632          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 11           |\n",
      "|    iterations           | 59           |\n",
      "|    time_elapsed         | 41141        |\n",
      "|    total_timesteps      | 483328       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053220512 |\n",
      "|    clip_fraction        | 0.229        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.05        |\n",
      "|    explained_variance   | 0.447        |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 5.58e+03     |\n",
      "|    n_updates            | 580          |\n",
      "|    policy_gradient_loss | 0.00245      |\n",
      "|    value_loss           | 1.14e+04     |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 115        |\n",
      "|    ep_rew_mean          | 687        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 11         |\n",
      "|    iterations           | 60         |\n",
      "|    time_elapsed         | 41234      |\n",
      "|    total_timesteps      | 491520     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00416396 |\n",
      "|    clip_fraction        | 0.23       |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | -1.08      |\n",
      "|    explained_variance   | 0.447      |\n",
      "|    learning_rate        | 1e-05      |\n",
      "|    loss                 | 4.39e+03   |\n",
      "|    n_updates            | 590        |\n",
      "|    policy_gradient_loss | 0.00372    |\n",
      "|    value_loss           | 1.05e+04   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 85.9        |\n",
      "|    ep_rew_mean          | 689         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 12          |\n",
      "|    iterations           | 61          |\n",
      "|    time_elapsed         | 41327       |\n",
      "|    total_timesteps      | 499712      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006719183 |\n",
      "|    clip_fraction        | 0.256       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.03       |\n",
      "|    explained_variance   | 0.372       |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 5.12e+03    |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | 0.00443     |\n",
      "|    value_loss           | 1.14e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 90.8        |\n",
      "|    ep_rew_mean          | 745         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 12          |\n",
      "|    iterations           | 62          |\n",
      "|    time_elapsed         | 41428       |\n",
      "|    total_timesteps      | 507904      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008727489 |\n",
      "|    clip_fraction        | 0.3         |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -0.96       |\n",
      "|    explained_variance   | 0.418       |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 4.65e+03    |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | 0.00539     |\n",
      "|    value_loss           | 1.33e+04    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 96.5         |\n",
      "|    ep_rew_mean          | 729          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 12           |\n",
      "|    iterations           | 63           |\n",
      "|    time_elapsed         | 41553        |\n",
      "|    total_timesteps      | 516096       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052829497 |\n",
      "|    clip_fraction        | 0.238        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -0.947       |\n",
      "|    explained_variance   | 0.43         |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 5.57e+03     |\n",
      "|    n_updates            | 620          |\n",
      "|    policy_gradient_loss | 0.00735      |\n",
      "|    value_loss           | 1.22e+04     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 85.8         |\n",
      "|    ep_rew_mean          | 730          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 12           |\n",
      "|    iterations           | 64           |\n",
      "|    time_elapsed         | 42817        |\n",
      "|    total_timesteps      | 524288       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0084994845 |\n",
      "|    clip_fraction        | 0.297        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -0.975       |\n",
      "|    explained_variance   | 0.464        |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 4.28e+03     |\n",
      "|    n_updates            | 630          |\n",
      "|    policy_gradient_loss | 0.00892      |\n",
      "|    value_loss           | 1.1e+04      |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 85.2        |\n",
      "|    ep_rew_mean          | 762         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 12          |\n",
      "|    iterations           | 65          |\n",
      "|    time_elapsed         | 43006       |\n",
      "|    total_timesteps      | 532480      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006983526 |\n",
      "|    clip_fraction        | 0.244       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -0.924      |\n",
      "|    explained_variance   | 0.446       |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 6.64e+03    |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | 0.00461     |\n",
      "|    value_loss           | 1.38e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 72.9        |\n",
      "|    ep_rew_mean          | 739         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 12          |\n",
      "|    iterations           | 66          |\n",
      "|    time_elapsed         | 43149       |\n",
      "|    total_timesteps      | 540672      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008203702 |\n",
      "|    clip_fraction        | 0.28        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -0.888      |\n",
      "|    explained_variance   | 0.458       |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 9.51e+03    |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | 0.00519     |\n",
      "|    value_loss           | 1.31e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 75.4        |\n",
      "|    ep_rew_mean          | 799         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 12          |\n",
      "|    iterations           | 67          |\n",
      "|    time_elapsed         | 43314       |\n",
      "|    total_timesteps      | 548864      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010053469 |\n",
      "|    clip_fraction        | 0.241       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -0.838      |\n",
      "|    explained_variance   | 0.451       |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 1.29e+04    |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | 0.0059      |\n",
      "|    value_loss           | 1.62e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 70          |\n",
      "|    ep_rew_mean          | 718         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 12          |\n",
      "|    iterations           | 68          |\n",
      "|    time_elapsed         | 43741       |\n",
      "|    total_timesteps      | 557056      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008123361 |\n",
      "|    clip_fraction        | 0.232       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -0.806      |\n",
      "|    explained_variance   | 0.407       |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 8.71e+03    |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | 0.00514     |\n",
      "|    value_loss           | 1.76e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 73.2        |\n",
      "|    ep_rew_mean          | 775         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 12          |\n",
      "|    iterations           | 69          |\n",
      "|    time_elapsed         | 44195       |\n",
      "|    total_timesteps      | 565248      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010038931 |\n",
      "|    clip_fraction        | 0.268       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -0.757      |\n",
      "|    explained_variance   | 0.459       |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 1.21e+04    |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | 0.00618     |\n",
      "|    value_loss           | 1.78e+04    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x23797712390>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = VizDoomGym(config = r'C:\\Users\\indug\\doom\\ViZDoom\\scenarios\\deadly_corridor_s2.cfg')\n",
    "model.set_env(env)\n",
    "model.learn(total_timesteps=560000, callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d9499c38-bff5-437c-9cb2-a888663c3e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = PPO.load('./train/train_corridor/best_model_440000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1e98fecc-2f4c-4f5e-a729-b37b58464aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create rendered environment\n",
    "env = VizDoomGym(render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b2d28fe2-325f-4fbc-a65a-40923efbacb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate mean reward for 10 games\n",
    "mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1276a5fc-c67b-4833-b760-a500799bc3fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-215.9957061767578"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c41e66c8-6f4e-49da-8932-075598b3b047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Reward for episode 0 is 1721.8127899169922\n",
      "Total Reward for episode 1 is -111.82392883300781\n",
      "Total Reward for episode 2 is 1161.8428497314453\n",
      "Total Reward for episode 3 is 688.308349609375\n",
      "Total Reward for episode 4 is 511.5319519042969\n",
      "Total Reward for episode 5 is 802.8285064697266\n",
      "Total Reward for episode 6 is 1125.2847137451172\n",
      "Total Reward for episode 7 is 235.8668670654297\n",
      "Total Reward for episode 8 is 644.769775390625\n",
      "Total Reward for episode 9 is 463.5858154296875\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for episode in range(10): \n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done: \n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        time.sleep(0.80)\n",
    "        total_reward += reward\n",
    "    print('Total Reward for episode {} is {}'.format(episode, total_reward))\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4971e0-32fa-4311-ba26-1391c4cbf4d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
